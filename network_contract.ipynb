{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "network_contract.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpf7U1M9Qm7u52RXkIUpRa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gevenbly/TensorAlgs/blob/main/network_contract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_7MunT8UgpE"
      },
      "source": [
        "# !git clone https://github.com/gevenbly/TensorAlgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOEugbeTVZ3m"
      },
      "source": [
        "# Import necessary modules\n",
        "# import numpy as np\n",
        "# from typing import List, Union, Tuple, Optional\n",
        "# import matplotlib.pyplot as plt\n",
        "# import os\n",
        "# os.chdir('/content/TensorAlgs')\n",
        "from network_render import draw_network\n",
        "from network_solve import full_solve_complete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fID5i2txp1MX"
      },
      "source": [
        "# chi = 6\n",
        "# chim = 4\n",
        "# u = np.random.rand(chi,chi,chim,chim)\n",
        "# w = np.random.rand(chim,chim,chi)\n",
        "# h = np.random.rand(chi,chi,chi,chi,chi,chi)\n",
        "# tensors = [u,u,w,w,w,h,u,u,w,w,w]\n",
        "# connects = [[1, 3, 10, 11], [4, 7, 12, 13], [8, 10, -4], [11, 12, -5],\n",
        "#             [13, 14, -6], [2, 5, 6, 3, 4, 7], [1, 2, 9, 17], [5, 6, 16, 15],\n",
        "#             [8, 9, -1], [17, 16, -2], [15, 14, -3]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v05zHV_qseAi"
      },
      "source": [
        "def xcon(tensors_in, connects_in, order=None, which_envs=None, allow_str=True, \n",
        "         check_validity=True, return_costs=False, solver=None):\n",
        "  \"\"\" \n",
        "  eXtreme CONtractor. Upgrades `ncon` in numerous ways, including (i) support\n",
        "  for network labels as strings, (ii) incorporating auto-differentiation to \n",
        "  determine the single tensor environments. \n",
        "  \"\"\"\n",
        "  # todo: final_order, partial trace\n",
        "\n",
        "  # duplicate input list\n",
        "  tensors = [tensor for tensor in tensors_in]\n",
        "\n",
        "  # normalize connection labels\n",
        "  if allow_str:\n",
        "    # build dictionary between original and canonical labels\n",
        "    connects, fwd_dict, rev_dict, npos, nneg = make_cannon_connects(connects_in)\n",
        "  else:\n",
        "    connects = [np.array(connect, dtype=int) for connect in connects_in]\n",
        "  flat_connects = np.concatenate(connects)\n",
        "  uni_connects = np.unique(flat_connects)\n",
        "\n",
        "  # normalize the order\n",
        "  if order is None:\n",
        "    order_new = uni_connects[uni_connects > 0]\n",
        "  else:\n",
        "    if allow_str:\n",
        "      order_new = np.array([fwd_dict[ele] for ele in order])\n",
        "    else:\n",
        "      order_new = np.array([ele for ele in order])\n",
        "\n",
        "  # check validity of network\n",
        "  if check_validity:\n",
        "    dims = [tensor.shape for tensor in tensors]\n",
        "    if allow_str:\n",
        "      check_inputs(connects, dims, order_new, rev_dict=rev_dict)\n",
        "    else:\n",
        "      check_inputs(connects, dims, order_new)\n",
        "\n",
        "  # compute contraction costs\n",
        "  if return_costs:\n",
        "    # identify contraction indices\n",
        "    pt_cont, bn_cont = identify_cont_labels(connects, order_new)\n",
        "\n",
        "    # compute contraction costs\n",
        "    pt_costs, bn_costs = compute_costs(connects, dims, pt_cont, bn_cont)\n",
        "    tot_costs = sum(pt_costs) + sum(bn_costs)\n",
        "\n",
        "  # solve for optimal contraction order\n",
        "  if solver is not None:\n",
        "    if solver == 'greedy':\n",
        "      max_branch = 1\n",
        "    elif solver == 'full':\n",
        "      max_branch = None\n",
        "    elif isinstance(solver,int):\n",
        "      max_branch = solver\n",
        "    order_new, _, is_optimal = ncon_solver(tensors, connects, max_branch)\n",
        "  \n",
        "  # do partial traces\n",
        "  \n",
        "  # check whether open and, if not, whether scalar\n",
        "  if min(flat_connects) <= 0:\n",
        "    is_open = True\n",
        "    is_scalar = False\n",
        "    if which_envs is not None:\n",
        "      raise ValueError(\n",
        "        'Calculations using `which_envs` not available for open networks.')\n",
        "    else:\n",
        "      tensors.append(np.ones(1))\n",
        "      connects.append(np.ones(0, dtype=int))\n",
        "      N = len(connects) \n",
        "      which_envs = [N-1]\n",
        "  else:\n",
        "    N = len(connects) \n",
        "    is_open = False\n",
        "    if which_envs is None:\n",
        "      is_scalar = True\n",
        "      which_envs = [0]\n",
        "      tensor_keep = tensors[0]\n",
        "    else:\n",
        "      is_scalar = False\n",
        "      if isinstance(which_envs, int):\n",
        "        which_envs = [which_envs] \n",
        "  \n",
        "  # find node representation of contraction\n",
        "  nodes = find_nodes(connects, order_new)\n",
        "  node_labs, needed_conts = reorder_nodes(nodes, which_envs)\n",
        "  common_nodes, temp_locs = np.intersect1d(node_labs, \n",
        "                                           2**np.arange(N, dtype=np.uint64), \n",
        "                                           assume_unique=True, \n",
        "                                           return_indices=True)[:2]\n",
        "  init_locs = np.intersect1d(common_nodes, 2**np.arange(N, dtype=np.uint64), \n",
        "                             assume_unique=True, return_indices=True)[2]\n",
        "  nodes_occupied = np.zeros(len(node_labs), dtype=int)\n",
        "  nodes_occupied[temp_locs] = np.ones(len(temp_locs))\n",
        "\n",
        "  # expand lists to include room for intermediate tensors\n",
        "  tensors_all = [np.zeros(0)] * len(node_labs)\n",
        "  connects_all = [0] * len(node_labs)\n",
        "  curr_nodes = 2**np.arange(N)\n",
        "  for count, loc in enumerate(temp_locs):\n",
        "    tensors_all[loc] = tensors[init_locs[count]]\n",
        "    connects_all[loc] = connects[init_locs[count]]\n",
        "  \n",
        "  # do all contractions\n",
        "  tensors_all, connects_all, node_labs = (\n",
        "      do_node_contracts(tensors_all, connects_all, nodes, needed_conts, \n",
        "                        node_labs, nodes_occupied))\n",
        "\n",
        "  # reorder output tensor list and do final permutation\n",
        "  if len(which_envs) == 1:\n",
        "    if is_open:\n",
        "      # take final ordering from normal order\n",
        "      perm_vecs = np.argsort(connects_all[0])[::-1]\n",
        "    else:\n",
        "      # take final ordering from removed tensor\n",
        "      _, a_perm, b_perm = np.intersect1d(connects_all[0], connects[which_envs[0]], return_indices=True)\n",
        "      perm_vecs = a_perm[b_perm]\n",
        "\n",
        "    tensors_all = np.transpose(tensors_all[0], perm_vecs)\n",
        "    if is_scalar:\n",
        "      # do final contraction to a scalar\n",
        "      ax = tuple(range(np.ndim(tensor_keep)))\n",
        "      tensors_all = np.tensordot(tensor_keep, tensors_all,axes=(ax,ax)).item()\n",
        "  else:\n",
        "    env_order = np.array([(2**N - 2**env - 1) for env in which_envs], \n",
        "                         dtype=np.uint64)\n",
        "    env_perm = [np.where(node_labs==env)[0].item() for env in env_order]\n",
        "    a_perm = []\n",
        "    b_perm = []\n",
        "    for k in range(len(which_envs)):\n",
        "      _, a_temp, b_temp = np.intersect1d(connects_all[env_perm[k]], \n",
        "                                         connects[which_envs[k]], \n",
        "                                         return_indices=True)\n",
        "      a_perm.append(a_temp)\n",
        "      b_perm.append(b_temp)\n",
        "\n",
        "    tensors_all[:] = [np.transpose(tensors_all[env_perm[k]],a_perm[k][b_perm[k]]\n",
        "        ) for k in range(len(env_perm))]\n",
        "\n",
        "  if solver is not None:\n",
        "    if return_costs:\n",
        "      return tensors_all, order_new, tot_costs\n",
        "    else:\n",
        "      return tensors_all, order_new\n",
        "  else:\n",
        "    if return_costs:\n",
        "      return tensors_all, tot_costs\n",
        "    else:\n",
        "      return tensors_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EI8sshDGYxM"
      },
      "source": [
        "def ncon_solver(tensors: List[np.ndarray],\n",
        "                labels: List[List[int]],\n",
        "                max_branch: Optional[int] = None):\n",
        "  \"\"\"\n",
        "  Solve for the contraction order of a tensor network (encoded in the `ncon`\n",
        "  syntax) that minimizes the computational cost.\n",
        "  Args:\n",
        "    tensors: list of the tensors in the network.\n",
        "    labels: list of the tensor connections (in standard `ncon` format).\n",
        "    max_branch: maximum number of contraction paths to search at each step.\n",
        "  Returns:\n",
        "    np.ndarray: the cheapest contraction order found (in ncon format).\n",
        "    float: the cost of the network contraction, given as log10(total_FLOPS).\n",
        "    bool: specifies if contraction order is guaranteed optimal.\n",
        "  \"\"\"\n",
        "  # build log-adjacency matrix\n",
        "  log_adj = ncon_to_adj(tensors, labels)\n",
        "\n",
        "  # run search algorithm\n",
        "  order, costs, is_optimal = full_solve_complete(log_adj, max_branch=max_branch)\n",
        "\n",
        "  # put contraction order back into ncon format\n",
        "  con_order = ord_to_ncon(labels, order)\n",
        "\n",
        "  return con_order, costs, is_optimal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_xrkSxWl6Jb"
      },
      "source": [
        "def reorder_nodes(nodes, which_envs):\n",
        "  \"\"\" \n",
        "  Find the set of node contractions required for all specified network \n",
        "  environments.\n",
        "  \"\"\"\n",
        "  \n",
        "  # Initializations\n",
        "  N = nodes.shape[0] + 2\n",
        "  node_labs = np.zeros((len(which_envs), 2 * (N - 2)), dtype=np.uint64)\n",
        "  pos_vec = 3*np.arange(N-2, dtype=np.uint64)\n",
        "\n",
        "  # transform nodes to string form (binary rep)\n",
        "  form = '0' + str(N+1) + 'b' \n",
        "  bin_nodes = [format(ele, form) for ele in list(nodes.flatten())]\n",
        "\n",
        "  # compute the collection of needed node contractions\n",
        "  needed_conts = np.zeros(3 * (N - 2), dtype=bool)\n",
        "  for count, env in enumerate(which_envs):\n",
        "    temp_locs = np.where(np.logical_not(np.array([int(ele[N - env]) \n",
        "      for ele in bin_nodes], dtype=bool)))[0]\n",
        "    node_labs[count,:] = nodes.flatten()[temp_locs]\n",
        "    temp_type = np.sum(np.mod(temp_locs, 3).reshape(N - 2, 2), \n",
        "                      axis=1).astype(np.uint64) - 1\n",
        "    needed_conts[temp_type + pos_vec] = np.ones(N - 2, dtype=bool)\n",
        "\n",
        "  # add final envs to the list of nodes\n",
        "  for env in which_envs:\n",
        "    node_labs = np.append(node_labs, np.uint64(2**N - 1 - 2**env))\n",
        "  node_labs = np.unique(node_labs.flatten())\n",
        "\n",
        "  return node_labs, needed_conts.reshape(N-2,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG9DJWdF6Kvw"
      },
      "source": [
        "def do_node_contracts(tensors, connects, nodes, needed_conts, node_labs, \n",
        "                      nodes_occupied):\n",
        "  \"\"\" \n",
        "  Contract all of the nodes needed to generate the full set of environments\n",
        "  \"\"\"\n",
        "\n",
        "  N = nodes.shape[0] + 2\n",
        "  while (sum(nodes_occupied) < len(nodes_occupied)):\n",
        "  \n",
        "    # find the tensor groups that we have possession of\n",
        "    poss_groups = node_labs[np.where(nodes_occupied)[0]]\n",
        "    temp_locs = np.intersect1d(nodes.flatten(), poss_groups, \n",
        "                                return_indices=True)[1]\n",
        "\n",
        "    # find the nodes contractions that we are able to do \n",
        "    poss_nodes = np.zeros(3 * (N-2), dtype=np.uint64)\n",
        "    poss_nodes[temp_locs] = np.ones(len(temp_locs))\n",
        "    poss_nodes = poss_nodes.reshape(N-2, 3)\n",
        "\n",
        "    possible_types = np.zeros(((N-2), 3), dtype=bool)\n",
        "    possible_types[:,0] = np.logical_and(poss_nodes[:,0], poss_nodes[:,1])\n",
        "    possible_types[:,1] = np.logical_and(poss_nodes[:,0], poss_nodes[:,2])\n",
        "    possible_types[:,2] = np.logical_and(poss_nodes[:,1], poss_nodes[:,2])\n",
        "\n",
        "    # find first node that we need to do and are able to do\n",
        "    nodes_to_do = np.logical_and(possible_types, needed_conts) # parallelize me!\n",
        "    xpos, ypos = np.divmod(np.where(nodes_to_do.flatten())[0][0], 3)\n",
        "    if ypos == 0:\n",
        "      ctype = [0, 1, 2]\n",
        "    elif ypos == 1:\n",
        "      ctype = [0, 2, 1]\n",
        "    elif ypos == 2:\n",
        "      ctype = [1, 2, 0]\n",
        "\n",
        "    # do node contractions\n",
        "    node_cont = np.array([nodes[xpos, ctype[0]], nodes[xpos, ctype[1]]], \n",
        "                          dtype=np.uint64)\n",
        "    node_cont = np.append(node_cont, sum(node_cont).astype(np.uint64))\n",
        "    node_locs = np.intersect1d(node_cont, node_labs, assume_unique=True, \n",
        "                                return_indices=True)[2]\n",
        "\n",
        "    # do binary contraction\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        connects[node_locs[0]], connects[node_locs[1]],\n",
        "        assume_unique=True, return_indices=True)\n",
        "    if np.size(tensors[node_locs[0]]) < np.size(tensors[node_locs[1]]):\n",
        "      ind_order = np.argsort(A_cont)\n",
        "    else:\n",
        "      ind_order = np.argsort(B_cont)\n",
        "\n",
        "    tensors[node_locs[2]] = np.tensordot(\n",
        "        tensors[node_locs[0]], tensors[node_locs[1]],\n",
        "        axes=(A_cont[ind_order], B_cont[ind_order]))\n",
        "    connects[node_locs[2]] = np.append(\n",
        "        np.delete(connects[node_locs[0]], A_cont),\n",
        "        np.delete(connects[node_locs[1]], B_cont))\n",
        "    \n",
        "    # update node information \n",
        "    needed_conts[xpos, ypos] = False\n",
        "    nodes_occupied[node_locs[2]] = 1\n",
        "\n",
        "    # identify intermediate tensors that are no longer needed\n",
        "    temp_loc0 = -1\n",
        "    temp_loc1 = -1\n",
        "    if ypos == 0:\n",
        "      if not needed_conts[xpos, 1]:\n",
        "        temp_loc0 = np.where(node_labs == nodes[xpos,0])[0][0]\n",
        "      if not needed_conts[xpos, 2]:\n",
        "        temp_loc1 = np.where(node_labs == nodes[xpos,1])[0][0]\n",
        "    elif ypos == 1:\n",
        "      if not needed_conts[xpos, 0]:\n",
        "        temp_loc0 = np.where(node_labs == nodes[xpos,0])[0][0]\n",
        "      if not needed_conts[xpos, 2]:\n",
        "        temp_loc1 = np.where(node_labs == nodes[xpos,2])[0][0]\n",
        "    elif ypos == 2:\n",
        "      if not needed_conts[xpos, 0]:\n",
        "        temp_loc0 = np.where(node_labs == nodes[xpos,1])[0][0]\n",
        "      if not needed_conts[xpos, 1]:\n",
        "        temp_loc1 = np.where(node_labs == nodes[xpos,2])[0][0]\n",
        "    \n",
        "    # delete intermidate tensors to free memory\n",
        "    if (temp_loc0 >= 0) and (temp_loc1 >= 0):\n",
        "      del tensors[max(temp_loc0, temp_loc1)]\n",
        "      del tensors[min(temp_loc0, temp_loc1)]\n",
        "      del connects[max(temp_loc0, temp_loc1)]\n",
        "      del connects[min(temp_loc0, temp_loc1)]\n",
        "      nodes_occupied = np.delete(nodes_occupied, max(temp_loc0, temp_loc1))\n",
        "      nodes_occupied = np.delete(nodes_occupied, min(temp_loc0, temp_loc1))\n",
        "      node_labs = np.delete(node_labs, max(temp_loc0, temp_loc1))\n",
        "      node_labs = np.delete(node_labs, min(temp_loc0, temp_loc1))\n",
        "    elif temp_loc0 >= 0:\n",
        "      del tensors[temp_loc0]\n",
        "      del connects[temp_loc0]\n",
        "      nodes_occupied = np.delete(nodes_occupied,temp_loc0)\n",
        "      node_labs = np.delete(node_labs, temp_loc0)\n",
        "    elif temp_loc1 >= 0:\n",
        "      del tensors[temp_loc1]\n",
        "      del connects[temp_loc1]\n",
        "      nodes_occupied = np.delete(nodes_occupied,temp_loc1)\n",
        "      node_labs = np.delete(node_labs, temp_loc1)\n",
        "      \n",
        "  return tensors, connects, node_labs\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRzkYV7Uyh1h"
      },
      "source": [
        "def partial_trace(A, A_label):\n",
        "  \"\"\" do partial trace on tensor A over repeated labels in A_label \"\"\"\n",
        "\n",
        "  num_cont = len(A_label) - len(np.unique(A_label))\n",
        "  if num_cont > 0:\n",
        "    dup_list = []\n",
        "    for ele in np.unique(A_label):\n",
        "      if sum(A_label == ele) > 1:\n",
        "        dup_list.append([np.where(A_label == ele)[0]])\n",
        "\n",
        "    cont_ind = np.array(dup_list).reshape(2 * num_cont, order='F')\n",
        "    free_ind = np.delete(np.arange(len(A_label)), cont_ind)\n",
        "\n",
        "    cont_dim = np.prod(np.array(A.shape)[cont_ind[:num_cont]])\n",
        "    free_dim = np.array(A.shape)[free_ind]\n",
        "\n",
        "    B_label = np.delete(A_label, cont_ind)\n",
        "    cont_label = np.unique(A_label[cont_ind])\n",
        "    B = np.zeros(np.prod(free_dim))\n",
        "    A = A.transpose(np.append(free_ind, cont_ind)).reshape(\n",
        "        np.prod(free_dim), cont_dim, cont_dim)\n",
        "    for ip in range(cont_dim):\n",
        "      B = B + A[:, ip, ip]\n",
        "\n",
        "    return B.reshape(free_dim), B_label, cont_label\n",
        "\n",
        "  else:\n",
        "    return A, A_label, []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vEITOLTUZWy"
      },
      "source": [
        "def remove_partial(connects, order, tensors=None):\n",
        "  \"\"\" \n",
        "  Remove indices corresponding to partial traces from `connects` and `order`.\n",
        "  Also can perform the partial trace on tensors if provided.\n",
        "  \"\"\"\n",
        "  tr_inds = []\n",
        "  red_connects = []\n",
        "  for count, connect in enumerate(connects):\n",
        "    # find the traced indices\n",
        "    unis, locs, invs, counts = np.unique(connect, return_index=True, \n",
        "                                         return_inverse=True, \n",
        "                                         return_counts=True)\n",
        "    c_inds = unis[invs[np.sort(locs[counts==2])]]\n",
        "    f_inds = unis[invs[np.sort(locs[counts==1])]]\n",
        "    \n",
        "    if tensors is not None:\n",
        "      # generate permutation \n",
        "      p0 = [np.where(connect==f_inds[k])[0].item() for k in range(len(f_inds))]\n",
        "      p1 = [np.where(connect==c_inds[k])[0][0].item() for k in range(len(c_inds))]\n",
        "      p2 = [np.where(connect==c_inds[k])[0][1].item() for k in range(len(c_inds))]\n",
        "      perm_vec = p0 + p1 + p2\n",
        "    \n",
        "      tensors_all = np.transpose(tensors[count], a_perm[b_perm])\n",
        "\n",
        "    red_connects.append(f_inds)\n",
        "    tr_inds.append(c_inds)\n",
        "\n",
        "  tr_inds = np.concatenate(tr_inds)\n",
        "  red_order = [lab for lab in order if sum(tr_inds==lab)==0]\n",
        "\n",
        "  return red_connects, red_order\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLpMHF5w1gkz"
      },
      "source": [
        "def find_nodes(connects, order):\n",
        "  \"\"\" \n",
        "  Find the node ordering of a network. \n",
        "  \"\"\"\n",
        "  tconnects = [connect for connect in connects]\n",
        "  torder = [ele for ele in order]\n",
        "  \n",
        "  N = len(tconnects) \n",
        "  temp_nodes = np.zeros(3, dtype=np.uint64)\n",
        "  nodes = np.zeros((N-2, 3), dtype=np.uint64)\n",
        "  node_labs = 2**(np.arange(N, dtype=np.uint64))\n",
        "  for count in range(N-2):\n",
        "    locs = [ele for ele in range(len(tconnects)) if \n",
        "            sum(tconnects[ele] == torder[0]) > 0]\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        tconnects[locs[0]],\n",
        "        tconnects[locs[1]],\n",
        "        assume_unique=True,\n",
        "        return_indices=True)\n",
        "\n",
        "    temp_nodes[0] = node_labs[locs[0]]\n",
        "    temp_nodes[1] = node_labs[locs[1]]\n",
        "    temp_nodes[2] = 2**N - node_labs[locs[0]] - node_labs[locs[1]] - 1\n",
        "    nodes[count, :] = np.sort(temp_nodes)\n",
        "\n",
        "    tconnects.append(np.concatenate((\n",
        "      np.delete(tconnects[locs[0]], A_cont),\n",
        "      np.delete(tconnects[locs[1]], B_cont))))\n",
        "    del tconnects[locs[1]]\n",
        "    del tconnects[locs[0]]\n",
        "\n",
        "    node_labs = np.append(node_labs, node_labs[locs[0]] + node_labs[locs[1]])\n",
        "    node_labs = np.delete(node_labs, [locs[0], locs[1]])\n",
        "\n",
        "    torder = [lab for lab in torder if sum(cont_many==lab)==0]\n",
        "\n",
        "  return nodes\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZQv28lHmLHh"
      },
      "source": [
        "def pre_ncon(connects, dims=None, order=None):\n",
        "  \"\"\" \n",
        "  Converts labels in `connects` to normal form (sequential integers). \n",
        "  Determines cost of contraction based on the given `order`.\n",
        "  \"\"\"\n",
        "  # define default dims everywhere as `d`\n",
        "  if dims is None:\n",
        "    dims = []\n",
        "    for tensor in connects:\n",
        "      dims.append(['d'] * len(tensor))\n",
        "\n",
        "  # build dictionary between original and canonical dims\n",
        "  nml_dims, fwd_dim_dict, rev_dim_dict = make_cannon_dims(dims)\n",
        "\n",
        "  # build dictionary between original and canonical labels\n",
        "  nml_connects, fwd_dict, rev_dict, npos, nneg = make_cannon_connects(connects)\n",
        "\n",
        "  # find canonical order\n",
        "  if order is None:\n",
        "    nml_order = np.arange(npos) + 1\n",
        "  else:\n",
        "    nml_order = np.array([fwd_dict[ele] for ele in order])\n",
        "\n",
        "  # check validity of network\n",
        "  check_inputs(nml_connects, nml_dims, nml_order, rev_dict, rev_dim_dict)\n",
        "\n",
        "  # identify contraction indices\n",
        "  pt_cont, bn_cont = identify_cont_labels(nml_connects, nml_order)\n",
        "\n",
        "  # compute contraction costs\n",
        "  pt_costs, bn_costs = compute_costs(nml_connects, nml_dims, pt_cont, bn_cont, \n",
        "                                     rev_dim_dict)\n",
        "    \n",
        "  return nml_connects, nml_order, fwd_dict, rev_dict, pt_cont, bn_cont, pt_costs, bn_costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOQc6iuqxQUM"
      },
      "source": [
        "def pre_ncon(connects, dims=None, order=None):\n",
        "  \"\"\" \n",
        "  Converts labels in `connects` to normal form (sequential integers). \n",
        "  Determines cost of contraction based on the given `order`.\n",
        "  \"\"\"\n",
        "  # define default dims everywhere as `d`\n",
        "  if dims is None:\n",
        "    dims = []\n",
        "    for tensor in connects:\n",
        "      dims.append(['d'] * len(tensor))\n",
        "\n",
        "  # build dictionary between original and canonical dims\n",
        "  nml_dims, fwd_dim_dict, rev_dim_dict = make_cannon_dims(dims)\n",
        "\n",
        "  # build dictionary between original and canonical labels\n",
        "  nml_connects, fwd_dict, rev_dict, npos, nneg = make_cannon_connects(connects)\n",
        "\n",
        "  # find canonical order\n",
        "  if order is None:\n",
        "    nml_order = np.arange(npos) + 1\n",
        "  else:\n",
        "    nml_order = np.array([fwd_dict[ele] for ele in order])\n",
        "\n",
        "  # check validity of network\n",
        "  check_inputs(nml_connects, nml_dims, nml_order, rev_dict, rev_dim_dict)\n",
        "\n",
        "  # identify contraction indices\n",
        "  pt_cont, bn_cont = identify_cont_labels(nml_connects, nml_order)\n",
        "\n",
        "  # compute contraction costs\n",
        "  pt_costs, bn_costs = compute_costs(nml_connects, nml_dims, pt_cont, bn_cont, \n",
        "                                     rev_dim_dict)\n",
        "    \n",
        "  return nml_connects, nml_order, fwd_dict, rev_dict, pt_cont, bn_cont, pt_costs, bn_costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP4sTameHvgi"
      },
      "source": [
        "def compute_costs(connects, dims, pt_cont, bn_cont, rev_dim_dict=None):\n",
        "  \"\"\" \n",
        "  Identify the labels involved in each tensor contraction (either a partial \n",
        "  trace or a binary tensor contraction).\n",
        "  \"\"\"\n",
        "  nml_connects = [ele for ele in connects]\n",
        "  nml_dims = [ele for ele in dims]\n",
        "\n",
        "  # partial trace costs\n",
        "  pt_costs = []\n",
        "  for pt_labs in pt_cont:\n",
        "    for count, sublist in enumerate(nml_connects):\n",
        "      pt_inds, pt_locs0, _ = np.intersect1d(sublist, pt_labs, return_indices=True)\n",
        "      if len(pt_inds) > 0:\n",
        "        sublist = np.delete(sublist, pt_locs0)\n",
        "        cont_cost = np.delete(nml_dims[count], pt_locs0)\n",
        "        _, pt_locs1, _ = np.intersect1d(sublist, pt_labs, return_indices=True)\n",
        "        \n",
        "        nml_connects[count] = np.delete(sublist, pt_locs1)\n",
        "        nml_dims[count] = np.delete(cont_cost, pt_locs1)\n",
        "        break\n",
        "    \n",
        "    pt_costs.append(cont_cost)\n",
        "\n",
        "  # binary contraction costs\n",
        "  bn_costs = []\n",
        "  for bn_labs in bn_cont:\n",
        "    locs = [ele for ele in range(len(nml_connects)) if \n",
        "            sum(nml_connects[ele] == bn_labs[0]) > 0]\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        nml_connects[locs[0]],\n",
        "        nml_connects[locs[1]],\n",
        "        assume_unique=True,\n",
        "        return_indices=True)\n",
        "\n",
        "    nml_connects.append(np.concatenate((\n",
        "        np.delete(nml_connects[locs[0]], A_cont),\n",
        "        np.delete(nml_connects[locs[1]], B_cont))))\n",
        "    bn_costs.append(np.concatenate((\n",
        "        np.delete(nml_dims[locs[0]], A_cont), nml_dims[locs[1]])))\n",
        "    nml_dims.append(np.concatenate((\n",
        "        np.delete(nml_dims[locs[0]], A_cont),\n",
        "        np.delete(nml_dims[locs[1]], B_cont))))\n",
        "\n",
        "    del nml_connects[locs[1]]\n",
        "    del nml_connects[locs[0]]\n",
        "    del nml_dims[locs[1]]\n",
        "    del nml_dims[locs[0]]\n",
        "\n",
        "  # tally the total partial trace costs\n",
        "  is_symbolic = False\n",
        "  int_pt_costs = []\n",
        "  fin_pt_costs = []\n",
        "  for cost in pt_costs:\n",
        "    uni_dims = np.unique(cost)\n",
        "\n",
        "    str_cost = ''\n",
        "    int_cost = 1\n",
        "    for dim in uni_dims:\n",
        "      degen = sum(cost == dim)\n",
        "      if rev_dim_dict is None:\n",
        "        value = dim\n",
        "      else:\n",
        "        value = rev_dim_dict[dim]\n",
        "\n",
        "      if isinstance(value, str):\n",
        "        str_cost += '(' + value + '^' + str(degen) + ')'\n",
        "        is_symbolic = True\n",
        "      else:\n",
        "        int_cost = int_cost * value**degen\n",
        "\n",
        "    int_pt_costs.append(int_cost)\n",
        "    if int_cost > 1:\n",
        "      if int_cost > 99:\n",
        "        fin_pt_costs.append(\"{:.1e}\".format(int_cost) + '*' + str_cost)\n",
        "      else:\n",
        "        fin_pt_costs.append(str(int_cost) + '*' + str_cost)\n",
        "    else:\n",
        "      fin_bn_costs.append(str_cost)\n",
        "\n",
        "  # tally the total binary contraction costs\n",
        "  int_bn_costs = []\n",
        "  fin_bn_costs = []\n",
        "  for cost in bn_costs:\n",
        "    uni_dims = np.unique(cost)\n",
        "\n",
        "    str_cost = ''\n",
        "    int_cost = 1\n",
        "    for dim in uni_dims:\n",
        "      degen = sum(cost == dim)\n",
        "      if rev_dim_dict is None:\n",
        "        value = dim\n",
        "      else:\n",
        "        value = rev_dim_dict[dim]\n",
        "\n",
        "      if isinstance(value, str):\n",
        "        str_cost += '(' + value + '^' + str(degen) + ')'\n",
        "        is_symbolic = True\n",
        "      else:\n",
        "        int_cost = int_cost * value**degen\n",
        "\n",
        "    int_bn_costs.append(int_cost)\n",
        "    if int_cost > 1:\n",
        "      if int_cost > 99:\n",
        "        fin_bn_costs.append(\"{:.1e}\".format(int_cost) + '*' + str_cost)\n",
        "      else:\n",
        "        fin_bn_costs.append(str(int_cost) + '*' + str_cost)\n",
        "    else:\n",
        "      fin_bn_costs.append(str_cost)\n",
        "\n",
        "  if not is_symbolic:\n",
        "    fin_pt_costs = int_pt_costs\n",
        "    fin_bn_costs = int_bn_costs\n",
        "\n",
        "  return fin_pt_costs, fin_bn_costs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErFGJMyZ17vB"
      },
      "source": [
        "def identify_cont_labels(connects, order):\n",
        "  \"\"\" \n",
        "  Identify the labels involved in each tensor contraction (either a partial \n",
        "  trace or a binary tensor contraction).\n",
        "  \"\"\"\n",
        "\n",
        "  nml_order = [ele for ele in order]\n",
        "  nml_connects = [ele for ele in connects]\n",
        "\n",
        "  # indentify partial trace indices to be contracted\n",
        "  pt_cont = []\n",
        "  for count, sublist in enumerate(nml_connects):\n",
        "    uni_labs, uni_locs = np.unique(sublist, return_index=True)\n",
        "    num_cont = len(sublist) - len(uni_labs)\n",
        "    if num_cont > 0:\n",
        "      dup_list = []\n",
        "      for ele in uni_labs:\n",
        "        temp_locs = np.where(sublist == ele)[0]\n",
        "        if len(temp_locs) == 2:\n",
        "          dup_list.append(ele)\n",
        "          sublist = np.delete(sublist, temp_locs)\n",
        "          nml_order = np.delete(nml_order, nml_order==ele)\n",
        "      \n",
        "      pt_cont.append(np.array(dup_list))\n",
        "      nml_connects[count] = sublist\n",
        "\n",
        "  # indentify binary contraction indices \n",
        "  bn_cont = []\n",
        "  while len(nml_order) > 0:\n",
        "    locs = [ele for ele in range(len(nml_connects)) \n",
        "            if sum(nml_connects[ele] == nml_order[0]) > 0]\n",
        "\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        nml_connects[locs[0]],\n",
        "        nml_connects[locs[1]],\n",
        "        assume_unique=True,\n",
        "        return_indices=True)\n",
        "    \n",
        "    bn_cont.append(cont_many)\n",
        "    nml_connects.append(np.concatenate((\n",
        "      np.delete(nml_connects[locs[0]], A_cont),\n",
        "      np.delete(nml_connects[locs[1]], B_cont))))\n",
        "    del nml_connects[locs[1]]\n",
        "    del nml_connects[locs[0]]\n",
        "    nml_order = np.delete(nml_order, np.intersect1d(nml_order, \n",
        "                                                    cont_many, \n",
        "                                                    return_indices=True)[1])\n",
        "    \n",
        "  return pt_cont, bn_cont"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekYqe5uX2Its"
      },
      "source": [
        "def make_cannon_dims(dims):\n",
        "  \"\"\" \n",
        "  Create dict holding the unique tensor dims, which may be input either as \n",
        "  strings or integers, and transform the dims according to this dict. \n",
        "  \"\"\"\n",
        "  \n",
        "  # flatten the list of connections\n",
        "  flat_dims = [item for sublist in dims for item in sublist]\n",
        "\n",
        "  # find unique entries\n",
        "  uni_dims = []\n",
        "  for ele in flat_dims:\n",
        "    if ele not in uni_dims:\n",
        "      uni_dims.append(ele)\n",
        "  \n",
        "  # create dictionary to map between original and cannonical dims\n",
        "  fwd_dict = dict(zip(uni_dims, np.arange(len(uni_dims))))\n",
        "  rev_dict = dict(zip(np.arange(len(uni_dims)), uni_dims))\n",
        "\n",
        "  # make canonical dims\n",
        "  can_dims = []\n",
        "  for tensor in dims:\n",
        "    temp_dims = []\n",
        "    for lab in tensor:\n",
        "      temp_dims.append(fwd_dict[lab])\n",
        "    can_dims.append(np.array(temp_dims, dtype=int))\n",
        "\n",
        "  return can_dims, fwd_dict, rev_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BmTxuZ9H19q"
      },
      "source": [
        "def make_cannon_connects(connects):\n",
        "  \"\"\"\n",
        "  Takes in a set of `connects` defining a network, where index labels can be\n",
        "  given either as `int` or `str` and returns dicts mapping between cannonical\n",
        "  labels: where open (external) indices are labelled with negative integers \n",
        "  (starting at -1) and closed (internal) indices are labelled with positive\n",
        "  integers (starting at +1). Sorting of indices is done alpha-numerically.\n",
        "  \"\"\"\n",
        "\n",
        "  # flatten the list of connections\n",
        "  connects = [list(connect) for connect in connects]\n",
        "  flat_connects = [item for sublist in connects for item in sublist]\n",
        "\n",
        "  # separate ints from strs\n",
        "  int_connects = []\n",
        "  str_connects = []\n",
        "  for ele in flat_connects:\n",
        "    if isinstance(ele, str):\n",
        "      str_connects.append(ele)\n",
        "    else:\n",
        "      int_connects.append(ele)\n",
        "    \n",
        "  # separate single (open) indices from double (closed) indices\n",
        "  sgl_str = []\n",
        "  dbl_str = []\n",
        "  for ele in str_connects:\n",
        "    if str_connects.count(ele) == 1:\n",
        "      sgl_str.append(ele)\n",
        "    elif str_connects.count(ele) == 2:\n",
        "      if dbl_str.count(ele) == 0:\n",
        "        dbl_str.append(ele)\n",
        "    else:\n",
        "      raise ValueError(\"index label {ind} is repeated more than twice\".format(\n",
        "          ind = \"`\" + ele + \"`\"))\n",
        "\n",
        "  sgl_int = []\n",
        "  dbl_int = []\n",
        "  for ele in int_connects:\n",
        "    if int_connects.count(ele) == 1:\n",
        "      sgl_int.append(ele)\n",
        "    elif int_connects.count(ele) == 2:\n",
        "      if dbl_int.count(ele) == 0:\n",
        "        dbl_int.append(ele)\n",
        "    else:\n",
        "      raise ValueError(\"index label {ind} is repeated more than twice\".format(\n",
        "          ind = \"`\" + str(ele) + \"`\"))\n",
        "  \n",
        "  # sort and combine index labels\n",
        "  sgl_str.sort()\n",
        "  dbl_str.sort()\n",
        "  sgl_int.sort()\n",
        "  sgl_int.reverse()\n",
        "  dbl_int.sort()\n",
        "  open_inds = sgl_str + sgl_int\n",
        "  clsd_inds = dbl_str + dbl_int\n",
        "  num_neg = len(open_inds)\n",
        "  num_pos = len(clsd_inds)\n",
        "\n",
        "  # create dictionary to map between original and cannonical labels\n",
        "  pos_labs = dict(zip(open_inds, -np.arange(1,len(open_inds) + 1)))\n",
        "  neg_labs = dict(zip(clsd_inds, np.arange(1,len(clsd_inds) + 1)))\n",
        "  can_labs = {**pos_labs, **neg_labs}\n",
        "  rev_can_labs = dict(zip(can_labs.values(), can_labs.keys()))\n",
        "\n",
        "  # make canonical connections\n",
        "  can_connects = []\n",
        "  for tensor in connects:\n",
        "    temp_inds = []\n",
        "    for lab in list(tensor):\n",
        "      temp_inds.append(can_labs[lab])\n",
        "    can_connects.append(np.array(temp_inds, dtype=int))\n",
        "\n",
        "  return can_connects, can_labs, rev_can_labs, num_pos, num_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFRdisfM6te2"
      },
      "source": [
        "def check_inputs(connects, dims, con_order, rev_dict=None, rev_dim_dict=None):\n",
        "  \"\"\" Check consistancy of NCON inputs\"\"\"\n",
        "\n",
        "  flat_connect = np.concatenate(connects)\n",
        "  pos_ind = flat_connect[flat_connect > 0]\n",
        "  neg_ind = flat_connect[flat_connect < 0]\n",
        "\n",
        "  # check that lengths of lists match\n",
        "  if len(dims) != len(connects):\n",
        "    raise ValueError((\n",
        "        'Network definition error: mismatch between {n0} tensors given but {n1}'\n",
        "        ' index sublists given'.format(n0 = str(len(dims)), \n",
        "                                       n1 = str(len(connects)))))\n",
        "\n",
        "  # check that tensors have the right number of indices\n",
        "  for ele in range(len(dims)):\n",
        "    if len(dims[ele]) != len(connects[ele]):\n",
        "      raise ValueError(\n",
        "          'Network definition error: number of indices does not match number'\n",
        "          ' of labels on tensor {n0}: {n1}-indices versus {n2}-labels'.format(\n",
        "              n0 = str(ele),\n",
        "              n1 = str(len(dims[ele])),\n",
        "              n2 = str(len(connects[ele]))))\n",
        "\n",
        "  # check that contraction order is valid\n",
        "  if not np.array_equal(np.sort(con_order), np.unique(pos_ind)):\n",
        "    raise ValueError('Network definition error: invalid contraction order')\n",
        "\n",
        "  # check that positive indices are valid and contracted tensor dimensions match\n",
        "  flat_dims = np.array([item for sublist in dims for item in sublist])\n",
        "  for ind in np.unique(pos_ind):\n",
        "    if sum(pos_ind == ind) == 1:\n",
        "      if rev_dict is not None:\n",
        "        ind_temp = rev_dict[ind]\n",
        "      else:\n",
        "        ind_temp = ind\n",
        "\n",
        "      raise ValueError(\n",
        "        'Network definition error: only one index labelled {n0}'\n",
        "        .format(n0 = \"`\" + str(ind_temp) + \"`\"))\n",
        "    elif sum(pos_ind == ind) > 2:\n",
        "      if rev_dict is not None:\n",
        "        ind_temp = rev_dict[ind]\n",
        "      else:\n",
        "        ind_temp = ind\n",
        "\n",
        "      raise ValueError(\n",
        "        'Network definition error: more than two indices labelled {n0}'\n",
        "        .format(n0 = \"`\" + str(ind_temp) + \"`\"))\n",
        "\n",
        "    cont_dims = flat_dims[flat_connect == ind]\n",
        "    if cont_dims[0] != cont_dims[1]:\n",
        "      if rev_dim_dict is not None:\n",
        "        d0 = rev_dim_dict[cont_dims[0]]\n",
        "        d1 = rev_dim_dict[cont_dims[1]]\n",
        "      else:\n",
        "        d0 = cont_dims[0]\n",
        "        d1 = cont_dims[1]\n",
        "\n",
        "      raise ValueError(\n",
        "          'Network definition error: tensor dimension mismatch on'\n",
        "          ' index labelled {n0}: dim-{n1} versus dim-{n2}'\n",
        "          .format(n0 = \"`\" + str(rev_dict[ind]) + \"`\", \n",
        "                  n1 = str(d0), \n",
        "                  n2 = str(d1)))\n",
        "\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FowB6YTjFzE4"
      },
      "source": [
        "def ord_to_ncon(labels: List[List[int]], orders: np.ndarray):\n",
        "  \"\"\"\n",
        "  Produces a `ncon` compatible index contraction order from the sequence of\n",
        "  pairwise contractions.\n",
        "  Args:\n",
        "    labels: list of the tensor connections (in standard `ncon` format).\n",
        "    orders: array of dim (2,N-1) specifying the set of N-1 pairwise\n",
        "      tensor contractions.\n",
        "  Returns:\n",
        "    np.ndarray: the contraction order (in `ncon` format).\n",
        "  \"\"\"\n",
        "\n",
        "  N = len(labels)\n",
        "  orders = orders.reshape(2, N - 1)\n",
        "  new_labels = [np.array(labels[i]) for i in range(N)]\n",
        "  con_order = np.zeros([0], dtype=int)\n",
        "\n",
        "  # remove all partial trace indices\n",
        "  for counter, temp_label in enumerate(new_labels):\n",
        "    uni_inds, counts = np.unique(temp_label, return_counts=True)\n",
        "    tr_inds = uni_inds[np.flatnonzero(counts == 2)]\n",
        "    con_order = np.concatenate((con_order, tr_inds))\n",
        "    new_labels[counter] = temp_label[np.isin(temp_label, uni_inds[counts == 1])]\n",
        "\n",
        "  for i in range(N - 1):\n",
        "    # find common indices between tensor pair\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        new_labels[orders[0, i]], new_labels[orders[1, i]], return_indices=True)\n",
        "    temp_labels = np.append(\n",
        "        np.delete(new_labels[orders[0, i]], A_cont),\n",
        "        np.delete(new_labels[orders[1, i]], B_cont))\n",
        "    con_order = list(np.concatenate((con_order, cont_many), axis=0))\n",
        "\n",
        "    # build new set of labels\n",
        "    new_labels[orders[0, i]] = temp_labels\n",
        "    del new_labels[orders[1, i]]\n",
        "\n",
        "  return con_order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Nc54_zF7uw"
      },
      "source": [
        "def ncon_to_adj(tensors: List[np.ndarray], labels: List[List[int]]):\n",
        "  \"\"\"\n",
        "  Create a log-adjacency matrix, where element [i,j] is the log10 of the total\n",
        "  dimension of the indices connecting ith and jth tensors, for a network\n",
        "  defined in the `ncon` syntax.\n",
        "  Args:\n",
        "    tensors: list of the tensors in the network.\n",
        "    labels: list of the tensor connections (in standard `ncon` format).\n",
        "  Returns:\n",
        "    np.ndarray: the log-adjacency matrix.\n",
        "  \"\"\"\n",
        "  # process inputs\n",
        "  N = len(labels)\n",
        "  ranks = [len(labels[i]) for i in range(N)]\n",
        "  flat_labels = np.hstack([labels[i] for i in range(N)])\n",
        "  tensor_counter = np.hstack(\n",
        "      [i * np.ones(ranks[i], dtype=int) for i in range(N)])\n",
        "  index_counter = np.hstack([np.arange(ranks[i]) for i in range(N)])\n",
        "\n",
        "  # build log-adjacency index-by-index\n",
        "  log_adj = np.zeros([N, N])\n",
        "  unique_labels = np.unique(flat_labels)\n",
        "  for ele in unique_labels:\n",
        "    # identify tensor/index location of each edge\n",
        "    tnr = tensor_counter[flat_labels == ele]\n",
        "    ind = index_counter[flat_labels == ele]\n",
        "    if len(ind) == 1:  # external index\n",
        "      log_adj[tnr[0], tnr[0]] += np.log10(tensors[tnr[0]].shape[ind[0]])\n",
        "    elif len(ind) == 2:  # internal index\n",
        "      if tnr[0] != tnr[1]:  # ignore partial traces\n",
        "        log_adj[tnr[0], tnr[1]] += np.log10(tensors[tnr[0]].shape[ind[0]])\n",
        "        log_adj[tnr[1], tnr[0]] += np.log10(tensors[tnr[0]].shape[ind[0]])\n",
        "\n",
        "  return log_adj"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}