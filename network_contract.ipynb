{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "network_contract.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPl9cNCOOzC5aDgTtheRifM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gevenbly/TensorAlgs/blob/main/network_contract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V101I58G5S8U"
      },
      "source": [
        "import numpy as np\n",
        "from typing import Optional, List, Union, Tuple\n",
        "from network_solve import (\n",
        "    full_solve_complete, ord_to_ncon, call_solver, ncon_to_weighted_adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R_R-xP0sCZo"
      },
      "source": [
        "def xcon(tensors_in, connects_in, order=None, open_order=None, which_envs=None, \n",
        "         standardize_inputs=True, perform_check=True, return_info=False, \n",
        "         solver=None):\n",
        "  \"\"\" \n",
        "  Xtreme CONtractor: Upgrades `ncon` in numerous ways, including (i) support\n",
        "  for network labels as strings, (ii) incorporation auto-differentiation to \n",
        "  determine the single tensor environments, (iii) incorporation of a solver to\n",
        "  find the optimal contraction order. Uses smart recycling of intermediate \n",
        "  tensors when computing multiple enviornments simultaneously, allowing for \n",
        "  a more efficient evaluation as compared to evaluating each environment \n",
        "  separately. \n",
        "  \"\"\"\n",
        "\n",
        "  # duplicate input list\n",
        "  tensors = [tensor for tensor in tensors_in]\n",
        "\n",
        "  # deal with single environment case separately\n",
        "  if isinstance(which_envs,list):\n",
        "    if len(which_envs) == 1:\n",
        "      which_envs = which_envs[0]\n",
        "  if isinstance(which_envs, int):\n",
        "    # generate new connects and order for network with vacancy\n",
        "    connects_new, order_new, open_ord = remove_tensor(\n",
        "        connects_in, which_envs, order=order)\n",
        "\n",
        "    # contract network with vacancy\n",
        "    del tensors[which_envs]\n",
        "    return xcon(tensors, connects_new, order=order_new, \n",
        "                standardize_inputs=False, perform_check=perform_check, \n",
        "                return_info=return_info, solver=solver, \n",
        "                open_order=open_order)\n",
        "  \n",
        "  # standardize connection labels\n",
        "  if standardize_inputs:\n",
        "    connects, fwd_dict, rev_dict, npos, nneg = (\n",
        "        make_canon_connects(connects_in, order=order, open_order=open_order))\n",
        "  else:\n",
        "    connects = [np.array(connect, dtype=int) for connect in connects_in]\n",
        "  flat_connects = np.concatenate(connects)\n",
        "  uni_connects = np.unique(flat_connects)\n",
        "\n",
        "  # normalize the order\n",
        "  if order is None:\n",
        "    order_new = uni_connects[uni_connects > 0]\n",
        "  else:\n",
        "    if standardize_inputs:\n",
        "      order_new = np.array([fwd_dict[ele] for ele in order], dtype=int)\n",
        "    else:\n",
        "      order_new = np.array([ele for ele in order], dtype=int)\n",
        "\n",
        "  # check validity of network\n",
        "  if perform_check:\n",
        "    dims = [tensor.shape for tensor in tensors]\n",
        "    if standardize_inputs:\n",
        "      check_network(connects, dims, order_new, rev_dict=rev_dict)\n",
        "    else:\n",
        "      check_network(connects, dims, order_new)\n",
        "\n",
        "  # solve for optimal contraction order\n",
        "  if solver is not None:\n",
        "    if solver == 'greedy':\n",
        "      max_branch = 1\n",
        "    elif solver == 'full':\n",
        "      max_branch = None\n",
        "    elif isinstance(solver, int):\n",
        "      max_branch = solver\n",
        "    order_new = solve_order(tensors, connects, max_branch, order=order_new)[0]\n",
        "\n",
        "  # use simple contraction scheme if envs are not required\n",
        "  if which_envs is None:\n",
        "    if standardize_inputs and return_info:\n",
        "      tensor_out, order_out, cost = ncon(\n",
        "          tensors, connects, order=order_new, perform_check=False, \n",
        "          return_info=return_info, standardize_inputs=False)\n",
        "      print(order_out)\n",
        "      print(rev_dict)\n",
        "      order_out = [rev_dict[ele] for ele in order_out]\n",
        "      print(order_out)\n",
        "      return tensor_out, order_out, cost\n",
        "    else:\n",
        "      return ncon(tensors, connects, order=order_new, perform_check=False,\n",
        "                  return_info=return_info)\n",
        "\n",
        "  # do all partial traces\n",
        "  tot_cost = 0\n",
        "  for ele in range(len(tensors)):\n",
        "    num_cont = len(connects[ele]) - len(np.unique(connects[ele]))\n",
        "    if num_cont > 0:\n",
        "      tensors[ele], connects[ele], cont_ind, cost = partial_trace(\n",
        "          connects[ele], tensor=tensors[ele])\n",
        "      order_new = np.delete(\n",
        "          order_new, np.intersect1d(\n",
        "              order_new, cont_ind, return_indices=True)[1])\n",
        "      tot_cost += cost\n",
        "      \n",
        "  # check whether open and, if not, whether scalar\n",
        "  N = len(connects) \n",
        "  if min(flat_connects) <= 0:\n",
        "    raise ValueError(\n",
        "        'Calculations using `which_envs` not available for open networks.')\n",
        "  else:\n",
        "    if which_envs is None:\n",
        "      is_scalar = True\n",
        "      which_envs = [0]\n",
        "      tensor_keep = tensors[0]\n",
        "    else:\n",
        "      is_scalar = False\n",
        "  \n",
        "  # find node representation of contraction\n",
        "  dims = [np.array(tensor.shape, dtype=int) for tensor in tensors]\n",
        "  nodes, temp_cost = find_nodes(connects, order_new, dims=dims)\n",
        "  tot_cost += temp_cost\n",
        "  node_labs, needed_conts = reorder_nodes(nodes, which_envs)\n",
        "  common_nodes, temp_locs = np.intersect1d(node_labs, \n",
        "                                           2**np.arange(N, dtype=np.uint64), \n",
        "                                           assume_unique=True, \n",
        "                                           return_indices=True)[:2]\n",
        "  init_locs = np.intersect1d(common_nodes, 2**np.arange(N, dtype=np.uint64), \n",
        "                             assume_unique=True, return_indices=True)[2]\n",
        "  nodes_occupied = np.zeros(len(node_labs), dtype=int)\n",
        "  nodes_occupied[temp_locs] = np.ones(len(temp_locs))\n",
        "\n",
        "  # expand lists to include room for intermediate tensors\n",
        "  tensors_all = [np.zeros(0)] * len(node_labs)\n",
        "  connects_all = [0] * len(node_labs)\n",
        "  curr_nodes = 2**np.arange(N)\n",
        "  for count, loc in enumerate(temp_locs):\n",
        "    tensors_all[loc] = tensors[init_locs[count]]\n",
        "    connects_all[loc] = connects[init_locs[count]]\n",
        "  \n",
        "  # do all contractions\n",
        "  tensors_all, connects_all, node_labs = (\n",
        "      do_node_contracts(tensors_all, connects_all, nodes, needed_conts, \n",
        "                        node_labs, nodes_occupied))\n",
        "\n",
        "  # reorder output tensor list and do final permutation\n",
        "  env_order = np.array([(2**N - 2**env - 1) for env in which_envs], \n",
        "                        dtype=np.uint64)\n",
        "  env_perm = [np.where(node_labs==env)[0].item() for env in env_order]\n",
        "  all_perms = []\n",
        "  for k in range(len(which_envs)):\n",
        "    perm_temp = [np.where(connects_all[env_perm[k]] == \n",
        "                          connects[which_envs[k]][p])[0].item() for \n",
        "                 p in range(len(connects[which_envs[k]]))] \n",
        "    all_perms.append(perm_temp)\n",
        "    \n",
        "  tensors_all[:] = [np.transpose(tensors_all[env_perm[k]], all_perms[k]) for \n",
        "                    k in range(len(env_perm))]\n",
        "  \n",
        "  if return_info:\n",
        "    if standardize_inputs:\n",
        "      order_new = [rev_dict[ele] for ele in order_new]\n",
        "    return tensors_all, order_new, tot_cost\n",
        "  else:\n",
        "    return tensors_all\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwQ7ns72cfLD"
      },
      "source": [
        "def find_nodes(connects, order, dims=None):\n",
        "  \"\"\" \n",
        "  Find the node ordering of a network. Requires std input of connects.\n",
        "  \"\"\"\n",
        "  tot_cost = 0;\n",
        "  tconnects = [np.array(connect, dtype=int) for connect in connects]\n",
        "  torder = [ele for ele in order]\n",
        "  \n",
        "  N = len(tconnects) \n",
        "  temp_nodes = np.zeros(3, dtype=np.uint64)\n",
        "  nodes = np.zeros((N-2, 3), dtype=np.uint64)\n",
        "  node_labs = 2**(np.arange(N, dtype=np.uint64))\n",
        "  for count in range(N-2):\n",
        "    locs = [ele for ele in range(len(tconnects)) if \n",
        "            sum(tconnects[ele] == torder[0]) > 0]\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        tconnects[locs[0]],\n",
        "        tconnects[locs[1]],\n",
        "        assume_unique=True,\n",
        "        return_indices=True)\n",
        "\n",
        "    temp_nodes[0] = node_labs[locs[0]]\n",
        "    temp_nodes[1] = node_labs[locs[1]]\n",
        "    temp_nodes[2] = 2**N - node_labs[locs[0]] - node_labs[locs[1]] - 1\n",
        "    nodes[count, :] = np.sort(temp_nodes)\n",
        "\n",
        "    # compute contraction costs\n",
        "    if dims is not None:\n",
        "      shp0 = np.array(dims[locs[0]], dtype=int)\n",
        "      shp1 = np.array(dims[locs[1]], dtype=int)\n",
        "      tot_cost += np.prod(shp0)*np.prod(shp1) // np.prod(\n",
        "          shp0[np.array(A_cont, dtype=int)])\n",
        "\n",
        "    tconnects.append(np.concatenate((\n",
        "      np.delete(tconnects[locs[0]], A_cont),\n",
        "      np.delete(tconnects[locs[1]], B_cont))))\n",
        "    del tconnects[locs[1]]\n",
        "    del tconnects[locs[0]]\n",
        "    if dims is not None:\n",
        "      dims.append(np.concatenate((\n",
        "        np.delete(dims[locs[0]], A_cont),\n",
        "        np.delete(dims[locs[1]], B_cont))))\n",
        "      del dims[locs[1]]\n",
        "      del dims[locs[0]]\n",
        "\n",
        "    node_labs = np.append(node_labs, node_labs[locs[0]] + node_labs[locs[1]])\n",
        "    node_labs = np.delete(node_labs, [locs[0], locs[1]])\n",
        "\n",
        "    torder = [lab for lab in torder if sum(cont_many==lab)==0]\n",
        "\n",
        "  return nodes, tot_cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvHYIU3ZbYNt"
      },
      "source": [
        "def node_to_order(connects_in, nodes, which_env):\n",
        "  \"\"\"\n",
        "  Produces a contraction order from the connects and nodes for the network with\n",
        "  tensor at `which_env` held vacant. Used in `ncon_remove`.\n",
        "  \"\"\"\n",
        "\n",
        "  N = len(connects_in)\n",
        "  connects = [connect for connect in connects_in]\n",
        "  del connects[which_env]\n",
        "\n",
        "  # initialize nodes\n",
        "  nodes_poss = [2**k for k in range(N) if k != which_env]\n",
        "  nodes_remain = list(range(N-2))\n",
        "  order = []\n",
        "  for k in range(N-2):\n",
        "    for p, pos in enumerate(nodes_remain):\n",
        "      # find nodes that are able to be contracted\n",
        "      nodes_avail = np.intersect1d(nodes_poss, nodes[pos,:])\n",
        "      if len(nodes_avail) == 2:\n",
        "        # update order and connects\n",
        "        loc = np.intersect1d(nodes_poss, nodes_avail, return_indices=True)[1]\n",
        "        ord_temp, loc0, loc1 = np.intersect1d(\n",
        "            connects[loc[0]], connects[loc[1]], return_indices=True)\n",
        "        order += list(ord_temp)\n",
        "        connects.append(list(np.concatenate((np.delete(connects[loc[0]], loc0), \n",
        "                            np.delete(connects[loc[1]], loc1)))))\n",
        "        \n",
        "        # update list of possessed nodes\n",
        "        nodes_poss.append(sum(nodes_avail))\n",
        "        del nodes_remain[p]       \n",
        "        break\n",
        "\n",
        "  return order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2-aOlmYKp8O"
      },
      "source": [
        "def solve_order(tensors: List[np.ndarray],\n",
        "                connects: List[List[int]],\n",
        "                max_branch: Optional[int] = None,\n",
        "                order: Optional[List[int]] = None):\n",
        "  \"\"\"\n",
        "  Solve for the contraction order of a tensor network (encoded in the `ncon`\n",
        "  syntax) that minimizes the computational cost.\n",
        "  Args:\n",
        "    tensors: list of the tensors in the network.\n",
        "    connects: list of the tensor connections (in standard `ncon` format).\n",
        "    max_branch: maximum number of contraction paths to search at each step.\n",
        "  Returns:\n",
        "    np.ndarray: the cheapest contraction order found (in ncon format).\n",
        "    float: the cost of the network contraction, given as log10(total_FLOPS).\n",
        "    bool: specifies if contraction order is guaranteed optimal.\n",
        "  \"\"\"\n",
        "  return call_solver(tensors, connects, max_branch=max_branch, order=order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiiL9fpuiuj8"
      },
      "source": [
        "def ncon(tensors:       List[np.ndarray],\n",
        "         connects:      List[Union[List[int], Tuple[int]]],\n",
        "         order:         Optional[Union[List[int], List[str]]] = None,\n",
        "         open_order:    Optional[Union[List[int], List[str]]] = None,\n",
        "         perform_check: Optional[bool] = True,\n",
        "         return_info:   Optional[bool] = False):\n",
        "  \"\"\"\n",
        "  Network CONtractor: contracts a tensor network of N tensors via a sequence\n",
        "  of (N-1) tensordot operations. More detailed instructions and examples can\n",
        "  be found at: https://arxiv.org/abs/1402.0939.\n",
        "  Args:\n",
        "    tensors: list of the tensors in the network.\n",
        "    connects: length-N list of lists (or tuples) specifying the network\n",
        "      connections. The jth entry of the ith list in connects labels the edge\n",
        "      connected to the jth index of the ith tensor. Labels should be positive\n",
        "      integers for internal indices and negative integers for free indices.\n",
        "    order: optional argument to specify the order for contracting the\n",
        "      positive indices. Defaults to ascending order if omitted. Can also be\n",
        "      set at \"greedy\" or \"full\" to call a solver to automatically determine\n",
        "      the order.\n",
        "    open_order: specification for the index ordering of the output tensor \n",
        "      (overrides the standard convention of descending order on -ve labels).\n",
        "    perform_check: if true then the input network is checked for consistency;\n",
        "      this can catch many common user mistakes for defining networks.\n",
        "    return info: if true then return the contraction `order` and `cost`.\n",
        "    \n",
        "  Returns:\n",
        "    Union[np.ndarray,float]: the result of the network contraction; an\n",
        "      np.ndarray if the network contained open indices, otherwise a scalar.\n",
        "  \"\"\"\n",
        "  tot_cost = 0\n",
        "  num_tensors = len(tensors)\n",
        "  tensor_list = [tensors[ele] for ele in range(num_tensors)]\n",
        "  connect_list = [np.array(connects[ele]) for ele in range(num_tensors)]\n",
        "\n",
        "  # generate contraction order if necessary\n",
        "  flat_connect = np.concatenate(connect_list)\n",
        "  if order is None:\n",
        "    order_new = np.unique(flat_connect[flat_connect > 0])\n",
        "  else:\n",
        "    order_new = np.array([ele for ele in list(order)], dtype=int)\n",
        "  order = [ele for ele in order_new]\n",
        "\n",
        "  # check inputs if enabled\n",
        "  if perform_check:\n",
        "    dims_list = [list(tensor.shape) for tensor in tensor_list]\n",
        "    check_network(connect_list, dims_list, order_new)\n",
        "\n",
        "  # do all partial traces\n",
        "  for ele in range(len(tensor_list)):\n",
        "    num_cont = len(connect_list[ele]) - len(np.unique(connect_list[ele]))\n",
        "    if num_cont > 0:\n",
        "      tensor_list[ele], connect_list[ele], cont_ind, cost = partial_trace(\n",
        "          connect_list[ele], tensor=tensor_list[ele])\n",
        "      order_new = np.delete(\n",
        "          order_new, np.intersect1d(order_new, cont_ind, return_indices=True)[1])\n",
        "      tot_cost += cost\n",
        "\n",
        "  # do all binary contractions\n",
        "  while len(order_new) > 0:\n",
        "    # identify tensors to be contracted\n",
        "    cont_ind = order_new[0]\n",
        "    locs = [ele for ele in range(len(connect_list)) if \n",
        "            sum(connect_list[ele] == cont_ind) > 0]\n",
        "\n",
        "    # identify indices to be contracted\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        connect_list[locs[0]],\n",
        "        connect_list[locs[1]],\n",
        "        assume_unique=True,\n",
        "        return_indices=True)\n",
        "    if np.size(tensor_list[locs[0]]) < np.size(tensor_list[locs[1]]):\n",
        "      ind_order = np.argsort(A_cont)\n",
        "    else:\n",
        "      ind_order = np.argsort(B_cont)\n",
        "\n",
        "    # compute contraction costs\n",
        "    shp0 = np.array(tensor_list[locs[0]].shape, dtype=int)\n",
        "    shp1 = np.array(tensor_list[locs[1]].shape, dtype=int)\n",
        "    tot_cost += np.prod(shp0)*np.prod(shp1) // np.prod(\n",
        "        shp0[np.array(A_cont, dtype=int)])\n",
        "\n",
        "    # do binary contraction using tensordot\n",
        "    tensor_list.append(\n",
        "        np.tensordot(\n",
        "            tensor_list[locs[0]],\n",
        "            tensor_list[locs[1]],\n",
        "            axes=(A_cont[ind_order], B_cont[ind_order])))\n",
        "    connect_list.append(\n",
        "        np.append(\n",
        "            np.delete(connect_list[locs[0]], A_cont),\n",
        "            np.delete(connect_list[locs[1]], B_cont)))\n",
        "\n",
        "    # remove contracted tensors from list and update order\n",
        "    del tensor_list[locs[1]]\n",
        "    del tensor_list[locs[0]]\n",
        "    del connect_list[locs[1]]\n",
        "    del connect_list[locs[0]]\n",
        "    order_new = np.delete(\n",
        "        order_new, np.intersect1d(order_new, cont_many, return_indices=True)[1])\n",
        "\n",
        "  # do all outer products\n",
        "  while len(tensor_list) > 1:\n",
        "    s1 = tensor_list[-2].shape\n",
        "    s2 = tensor_list[-1].shape\n",
        "    cost += np.prod(s1)*np.prod(s2)\n",
        "    tensor_list[-2] = np.outer(tensor_list[-2].reshape(np.prod(s1)),\n",
        "                               tensor_list[-1].reshape(np.prod(s2))).reshape(\n",
        "                                   np.append(s1, s2))\n",
        "    connect_list[-2] = np.append(connect_list[-2], connect_list[-1])\n",
        "    del tensor_list[-1]\n",
        "    del connect_list[-1]\n",
        "\n",
        "  # do final permutation\n",
        "  if len(connect_list[0]) > 0:\n",
        "    if open_order is None:\n",
        "      # default to conventional order\n",
        "      tensor_out =  np.transpose(tensor_list[0], np.argsort(-connect_list[0]))\n",
        "    else:\n",
        "      # custom index order\n",
        "      fin_perm = [np.where(connect_list[0]==lab)[0].item() for lab in open_order]\n",
        "      tensor_out = np.transpose(tensor_list[0], fin_perm)\n",
        "  else:\n",
        "    # export 0-dim ndarray into scalar\n",
        "    tensor_out = tensor_list[0].item()\n",
        "  \n",
        "  if return_info:\n",
        "    return tensor_out, order, tot_cost\n",
        "  else:\n",
        "    return tensor_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VnChGTcilYV"
      },
      "source": [
        "def partial_trace(labels, tensor=None):\n",
        "  \"\"\" \n",
        "  Partial trace on `tensor` over matching `labels`. If no tensor is provided \n",
        "  then only the labels are updated to remove repeat entries. Returns the new\n",
        "  tensor, its labels, the list of indices that were traced over, and the cost.\n",
        "  \"\"\"\n",
        "\n",
        "  cost = 0\n",
        "  labels = np.array(labels, dtype=int)\n",
        "  num_cont = len(labels) - len(np.unique(labels))\n",
        "  if num_cont > 0:\n",
        "    # determine indices to trace\n",
        "    dup_list = []\n",
        "    for ele in np.unique(labels):\n",
        "      if sum(labels == ele) > 1:\n",
        "        dup_list.append([np.where(labels == ele)[0]])\n",
        "    cont_ind = np.array(dup_list).reshape(2 * num_cont, order='F')\n",
        "    free_ind = np.delete(np.arange(len(labels)), cont_ind)\n",
        "\n",
        "    # labels of final tensor\n",
        "    B_label = np.delete(labels, cont_ind)\n",
        "    cont_label = np.unique(labels[cont_ind])\n",
        "\n",
        "    # do partial trace as an index summation\n",
        "    if tensor is not None:\n",
        "      # dimensions of indices to trace\n",
        "      cont_dim = np.prod(np.array(tensor.shape)[cont_ind[:num_cont]])\n",
        "      free_dim = np.array(tensor.shape)[free_ind]\n",
        "\n",
        "      cost = np.prod(free_dim)*cont_dim\n",
        "\n",
        "      B = np.zeros(np.prod(free_dim))\n",
        "      tensor = tensor.transpose(np.append(free_ind, cont_ind)).reshape(\n",
        "          np.prod(free_dim), cont_dim, cont_dim)\n",
        "      for ip in range(cont_dim):\n",
        "        B = B + tensor[:, ip, ip]\n",
        "\n",
        "      return B.reshape(free_dim), B_label, cont_label, cost\n",
        "    else:\n",
        "      return B_label, cont_label, cost\n",
        "\n",
        "  else:\n",
        "    # no partial trace is needed\n",
        "    return tensor, labels, [], cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMO7fWGziXxl"
      },
      "source": [
        "def do_node_contracts(tensors, connects, nodes, needed_conts, node_labs, \n",
        "                      nodes_occupied):\n",
        "  \"\"\" \n",
        "  Contract all of the nodes needed to generate the full set of environments\n",
        "  \"\"\"\n",
        "\n",
        "  N = nodes.shape[0] + 2\n",
        "  while (sum(nodes_occupied) < len(nodes_occupied)):\n",
        "  \n",
        "    # find the tensor groups that we have possession of\n",
        "    poss_groups = node_labs[np.where(nodes_occupied)[0]]\n",
        "    temp_locs = np.intersect1d(nodes.flatten(), poss_groups, \n",
        "                                return_indices=True)[1]\n",
        "\n",
        "    # find the nodes contractions that we are able to do \n",
        "    poss_nodes = np.zeros(3 * (N-2), dtype=np.uint64)\n",
        "    poss_nodes[temp_locs] = np.ones(len(temp_locs))\n",
        "    poss_nodes = poss_nodes.reshape(N-2, 3)\n",
        "\n",
        "    possible_types = np.zeros(((N-2), 3), dtype=bool)\n",
        "    possible_types[:,0] = np.logical_and(poss_nodes[:,0], poss_nodes[:,1])\n",
        "    possible_types[:,1] = np.logical_and(poss_nodes[:,0], poss_nodes[:,2])\n",
        "    possible_types[:,2] = np.logical_and(poss_nodes[:,1], poss_nodes[:,2])\n",
        "\n",
        "    # find first node that we need to do and are able to do\n",
        "    nodes_to_do = np.logical_and(possible_types, needed_conts) # parallelize me!\n",
        "    xpos, ypos = np.divmod(np.where(nodes_to_do.flatten())[0][0], 3)\n",
        "    if ypos == 0:\n",
        "      ctype = [0, 1, 2]\n",
        "    elif ypos == 1:\n",
        "      ctype = [0, 2, 1]\n",
        "    elif ypos == 2:\n",
        "      ctype = [1, 2, 0]\n",
        "\n",
        "    # do node contractions\n",
        "    node_cont = np.array([nodes[xpos, ctype[0]], nodes[xpos, ctype[1]]], \n",
        "                          dtype=np.uint64)\n",
        "    node_cont = np.append(node_cont, sum(node_cont).astype(np.uint64))\n",
        "    node_locs = np.intersect1d(node_cont, node_labs, assume_unique=True, \n",
        "                                return_indices=True)[2]\n",
        "\n",
        "    # do binary contraction\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        connects[node_locs[0]], connects[node_locs[1]],\n",
        "        assume_unique=True, return_indices=True)\n",
        "    if np.size(tensors[node_locs[0]]) < np.size(tensors[node_locs[1]]):\n",
        "      ind_order = np.argsort(A_cont)\n",
        "    else:\n",
        "      ind_order = np.argsort(B_cont)\n",
        "\n",
        "    tensors[node_locs[2]] = np.tensordot(\n",
        "        tensors[node_locs[0]], tensors[node_locs[1]],\n",
        "        axes=(A_cont[ind_order], B_cont[ind_order]))\n",
        "    connects[node_locs[2]] = np.append(\n",
        "        np.delete(connects[node_locs[0]], A_cont),\n",
        "        np.delete(connects[node_locs[1]], B_cont))\n",
        "    \n",
        "    # update node information \n",
        "    needed_conts[xpos, ypos] = False\n",
        "    nodes_occupied[node_locs[2]] = 1\n",
        "\n",
        "    # identify intermediate tensors that are no longer needed\n",
        "    temp_loc0 = -1\n",
        "    temp_loc1 = -1\n",
        "    if ypos == 0:\n",
        "      if not needed_conts[xpos, 1]:\n",
        "        temp_loc0 = np.where(node_labs == nodes[xpos,0])[0][0]\n",
        "      if not needed_conts[xpos, 2]:\n",
        "        temp_loc1 = np.where(node_labs == nodes[xpos,1])[0][0]\n",
        "    elif ypos == 1:\n",
        "      if not needed_conts[xpos, 0]:\n",
        "        temp_loc0 = np.where(node_labs == nodes[xpos,0])[0][0]\n",
        "      if not needed_conts[xpos, 2]:\n",
        "        temp_loc1 = np.where(node_labs == nodes[xpos,2])[0][0]\n",
        "    elif ypos == 2:\n",
        "      if not needed_conts[xpos, 0]:\n",
        "        temp_loc0 = np.where(node_labs == nodes[xpos,1])[0][0]\n",
        "      if not needed_conts[xpos, 1]:\n",
        "        temp_loc1 = np.where(node_labs == nodes[xpos,2])[0][0]\n",
        "    \n",
        "    # delete intermidate tensors to free memory\n",
        "    if (temp_loc0 >= 0) and (temp_loc1 >= 0):\n",
        "      del tensors[max(temp_loc0, temp_loc1)]\n",
        "      del tensors[min(temp_loc0, temp_loc1)]\n",
        "      del connects[max(temp_loc0, temp_loc1)]\n",
        "      del connects[min(temp_loc0, temp_loc1)]\n",
        "      nodes_occupied = np.delete(nodes_occupied, max(temp_loc0, temp_loc1))\n",
        "      nodes_occupied = np.delete(nodes_occupied, min(temp_loc0, temp_loc1))\n",
        "      node_labs = np.delete(node_labs, max(temp_loc0, temp_loc1))\n",
        "      node_labs = np.delete(node_labs, min(temp_loc0, temp_loc1))\n",
        "    elif temp_loc0 >= 0:\n",
        "      del tensors[temp_loc0]\n",
        "      del connects[temp_loc0]\n",
        "      nodes_occupied = np.delete(nodes_occupied,temp_loc0)\n",
        "      node_labs = np.delete(node_labs, temp_loc0)\n",
        "    elif temp_loc1 >= 0:\n",
        "      del tensors[temp_loc1]\n",
        "      del connects[temp_loc1]\n",
        "      nodes_occupied = np.delete(nodes_occupied,temp_loc1)\n",
        "      node_labs = np.delete(node_labs, temp_loc1)\n",
        "      \n",
        "  return tensors, connects, node_labs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRqUU8LW6XFJ"
      },
      "source": [
        "def remove_tensor(connects_in, which_env, order=None, open_order=None,\n",
        "                  standardize_outputs=True, one_based=False):\n",
        "  \"\"\"\n",
        "  Given a closed network and contraction order as input, will output a network \n",
        "  and new contraction order for the single tensor environment specified by the \n",
        "  input `which_env`. The new contraction order is gauranteed to be of lesser \n",
        "  or equivalent cost to that of the input order. Output network can either be\n",
        "  standardized (+ve ints for internal indices and -ve ints for open indices) or\n",
        "  retain the original labels. Open indices can either use zero-based numbering\n",
        "  (default) or one-based numbering.\n",
        "  \"\"\"\n",
        " \n",
        "  # build dictionary between original and canonical labels\n",
        "  N = len(connects_in)\n",
        "  connects, fwd_dict, rev_dict, npos, nneg = make_canon_connects(connects_in)\n",
        "  flat_connects = np.concatenate(connects)\n",
        "  uni_connects = np.unique(flat_connects)\n",
        "\n",
        "  # check that original network is closed\n",
        "  if min(flat_connects) <= 0:\n",
        "    raise ValueError(\n",
        "      'This function is only applicable for closed tensor networks.')\n",
        "\n",
        "  # normalize the order\n",
        "  if order is None:\n",
        "    order_new = uni_connects[uni_connects > 0]\n",
        "  else:\n",
        "    order_new = np.array([fwd_dict[ele] for ele in order])\n",
        "  \n",
        "  # find the new order of contraction\n",
        "  nodes = find_nodes(connects, order_new)[0]\n",
        "  order_out = node_to_order(connects, nodes, which_env)\n",
        "\n",
        "  if standardize_outputs:\n",
        "    # put connect labels into standardized form \n",
        "    fin_connects = [connect for connect in connects]\n",
        "    temp_ord = fin_connects.pop(which_env)\n",
        "    for p, connect in enumerate(fin_connects):\n",
        "      comm, loc0, loc1 = np.intersect1d(connect, temp_ord, return_indices=True)\n",
        "      for k in range(len(comm)):\n",
        "        fin_connects[p][loc0[k]] = -loc1[k] - one_based\n",
        "      fin_connects[p] = list(fin_connects[p])\n",
        "    open_ord = list(range(-one_based, -len(fin_connects)-one_based, -1))\n",
        "  else:\n",
        "    # put connect labels back into original form \n",
        "    order_out = [rev_dict[order_out[k]] for k in range(len(order_out))]\n",
        "    fin_connects = [connect for connect in connects_in]\n",
        "    open_ord = fin_connects.pop(which_env)\n",
        "    \n",
        "  return fin_connects, order_out, open_ord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmmTo20A2viy"
      },
      "source": [
        "def reorder_nodes(nodes, which_envs):\n",
        "  \"\"\" \n",
        "  Find the set of node contractions required for all specified network \n",
        "  environments.\n",
        "  \"\"\"\n",
        "  \n",
        "  if isinstance(which_envs, int):\n",
        "    which_envs = [which_envs]\n",
        "\n",
        "  # Initializations\n",
        "  N = nodes.shape[0] + 2\n",
        "  node_labs = np.zeros((len(which_envs), 2 * (N - 2)), dtype=np.uint64)\n",
        "  pos_vec = 3*np.arange(N-2, dtype=np.uint64)\n",
        "\n",
        "  # transform nodes to string form (binary rep)\n",
        "  form = '0' + str(N+1) + 'b' \n",
        "  bin_nodes = [format(ele, form) for ele in list(nodes.flatten())]\n",
        "\n",
        "  # compute the collection of needed node contractions\n",
        "  needed_conts = np.zeros(3 * (N - 2), dtype=bool)\n",
        "  for count, env in enumerate(which_envs):\n",
        "    temp_locs = np.where(np.logical_not(np.array([int(ele[N - env]) \n",
        "      for ele in bin_nodes], dtype=bool)))[0]\n",
        "    node_labs[count,:] = nodes.flatten()[temp_locs]\n",
        "    temp_type = np.sum(np.mod(temp_locs, 3).reshape(N - 2, 2), \n",
        "                      axis=1).astype(np.uint64) - 1\n",
        "    needed_conts[temp_type + pos_vec] = np.ones(N - 2, dtype=bool)\n",
        "\n",
        "  # add final envs to the list of nodes\n",
        "  for env in which_envs:\n",
        "    node_labs = np.append(node_labs, np.uint64(2**N - 1 - 2**env))\n",
        "  node_labs = np.unique(node_labs.flatten())\n",
        "\n",
        "  return node_labs, needed_conts.reshape(N-2,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAGVw4ma8UG5"
      },
      "source": [
        "def make_canon_connects(connects, order=None, open_order=None, one_based=False):\n",
        "  \"\"\"\n",
        "  Takes in a set of `connects` defining a network, where index labels can be\n",
        "  given either as `int` or `str` and returns dicts mapping between cannonical\n",
        "  labels: where open (external) indices are labelled with negative integers \n",
        "  (starting at -'start') and closed (internal) indices are labelled with \n",
        "  positive integers (starting at +1). Sorting of indices of internal and open\n",
        "  indices can either be provided (via 'order' and 'open_order') or will default\n",
        "  to strings (alphabetical order) followed by integers (ascending order). Also \n",
        "  returns dictionaries for transforming between original and canonical indices.\n",
        "  \"\"\"\n",
        "\n",
        "  # flatten the list of connections\n",
        "  connects = [list(connect) for connect in connects]\n",
        "  flat_connects = [item for sublist in connects for item in sublist]\n",
        "\n",
        "  # separate ints from strs\n",
        "  int_connects = []\n",
        "  str_connects = []\n",
        "  for ele in flat_connects:\n",
        "    if isinstance(ele, str):\n",
        "      str_connects.append(ele)\n",
        "    else:\n",
        "      int_connects.append(ele)\n",
        "    \n",
        "  # separate single (open) indices from double (closed) indices\n",
        "  sgl_str = []\n",
        "  dbl_str = []\n",
        "  for ele in str_connects:\n",
        "    if str_connects.count(ele) == 1:\n",
        "      sgl_str.append(ele)\n",
        "    elif str_connects.count(ele) == 2:\n",
        "      if dbl_str.count(ele) == 0:\n",
        "        dbl_str.append(ele)\n",
        "    else:\n",
        "      raise ValueError(\"index label {ind} is repeated more than twice\".format(\n",
        "          ind = \"`\" + ele + \"`\"))\n",
        "\n",
        "  sgl_int = []\n",
        "  dbl_int = []\n",
        "  for ele in int_connects:\n",
        "    if int_connects.count(ele) == 1:\n",
        "      sgl_int.append(ele)\n",
        "    elif int_connects.count(ele) == 2:\n",
        "      if dbl_int.count(ele) == 0:\n",
        "        dbl_int.append(ele)\n",
        "    else:\n",
        "      raise ValueError(\"index label {ind} is repeated more than twice\".format(\n",
        "          ind = \"`\" + str(ele) + \"`\"))\n",
        "  \n",
        "  # sort and combine index labels\n",
        "  if order is None:\n",
        "    # use standard order\n",
        "    dbl_str.sort()\n",
        "    dbl_int.sort()\n",
        "    clsd_inds = dbl_str + dbl_int\n",
        "  else:\n",
        "    # user-defined order\n",
        "    clsd_inds = order\n",
        "\n",
        "  if open_order is None:\n",
        "    # use standard order\n",
        "    sgl_str.sort()\n",
        "    sgl_int.sort()\n",
        "    sgl_int.reverse()\n",
        "    open_inds = sgl_str + sgl_int\n",
        "  else:\n",
        "    # user-defined order\n",
        "    open_inds = open_order\n",
        "\n",
        "  num_pos = len(clsd_inds)\n",
        "  num_neg = len(open_inds)\n",
        "\n",
        "  # create dictionary to map between original and cannonical labels\n",
        "  neg_labs = dict(zip(open_inds, -np.arange(one_based,len(open_inds) + \n",
        "                                            one_based)))\n",
        "  pos_labs = dict(zip(clsd_inds, np.arange(1,len(clsd_inds) + 1)))\n",
        "  fwd_dict = {**neg_labs, **pos_labs}\n",
        "  rev_dict = dict(zip(fwd_dict.values(), fwd_dict.keys()))\n",
        "\n",
        "  # make canonical connections\n",
        "  can_connects = []\n",
        "  for tensor in connects:\n",
        "    temp_inds = []\n",
        "    for lab in list(tensor):\n",
        "      temp_inds.append(fwd_dict[lab])\n",
        "    can_connects.append(np.array(temp_inds, dtype=int))\n",
        "\n",
        "  return can_connects, fwd_dict, rev_dict, num_pos, num_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxQi_7_l4qUj"
      },
      "source": [
        "def make_canon_dims(dims):\n",
        "  \"\"\" \n",
        "  Create dict holding the unique tensor dims, which may be input either as \n",
        "  strings or integers, and transform the dims according to this dict. \n",
        "  \"\"\"\n",
        "  \n",
        "  # flatten the list of connections\n",
        "  flat_dims = [item for sublist in dims for item in sublist]\n",
        "\n",
        "  # find unique entries\n",
        "  uni_dims = []\n",
        "  for ele in flat_dims:\n",
        "    if ele not in uni_dims:\n",
        "      uni_dims.append(ele)\n",
        "  \n",
        "  # create dictionary to map between original and cannonical dims\n",
        "  fwd_dict = dict(zip(uni_dims, np.arange(len(uni_dims))))\n",
        "  rev_dict = dict(zip(np.arange(len(uni_dims)), uni_dims))\n",
        "\n",
        "  # make canonical dims\n",
        "  can_dims = []\n",
        "  for tensor in dims:\n",
        "    temp_dims = []\n",
        "    for lab in tensor:\n",
        "      temp_dims.append(fwd_dict[lab])\n",
        "    can_dims.append(np.array(temp_dims, dtype=int))\n",
        "\n",
        "  return can_dims, fwd_dict, rev_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSJsISIc3aLT"
      },
      "source": [
        "def compute_costs(connects, order=None, dims=None, return_pt=False):\n",
        "  \"\"\" \n",
        "  Computes the cost of the sequence of binary contractions (and optionally also\n",
        "  the partial traces) require to contract a network given some `order`. Input\n",
        "  `dims` can be symbolic (strings) or numeric (ints), or a combination of both.\n",
        "  \"\"\"\n",
        "\n",
        "  # define default dims everywhere as `d`\n",
        "  if dims is None:\n",
        "    dims = []\n",
        "    for tensor in connects:\n",
        "      dims.append(['d'] * len(tensor))\n",
        "\n",
        "  # build dictionary between original and canonical dims\n",
        "  nml_dims, fwd_dim_dict, rev_dim_dict = make_canon_dims(dims)\n",
        "  \n",
        "  # build dictionary between original and canonical dims\n",
        "  nml_connects, fwd_dict, _, npos, num_neg = make_canon_connects(connects)\n",
        "  keep_connects = [connect for connect in nml_connects]\n",
        "\n",
        "  # find canonical order\n",
        "  if order is None:\n",
        "    nml_order = np.arange(npos) + 1\n",
        "  else:\n",
        "    nml_order = np.array([fwd_dict[ele] for ele in order])\n",
        "\n",
        "  # indentify partial trace indices to be contracted\n",
        "  pt_cont = []\n",
        "  for count, sublist in enumerate(nml_connects):\n",
        "    uni_labs, uni_locs = np.unique(sublist, return_index=True)\n",
        "    num_cont = len(sublist) - len(uni_labs)\n",
        "    if num_cont > 0:\n",
        "      dup_list = []\n",
        "      for ele in uni_labs:\n",
        "        temp_locs = np.where(sublist == ele)[0]\n",
        "        if len(temp_locs) == 2:\n",
        "          dup_list.append(ele)\n",
        "          sublist = np.delete(sublist, temp_locs)\n",
        "          nml_order = np.delete(nml_order, nml_order==ele)\n",
        "      \n",
        "      pt_cont.append(np.array(dup_list))\n",
        "      nml_connects[count] = sublist\n",
        "\n",
        "  # indentify binary contraction indices \n",
        "  bn_cont = []\n",
        "  while len(nml_order) > 0:\n",
        "    locs = [ele for ele in range(len(nml_connects)) \n",
        "            if sum(nml_connects[ele] == nml_order[0]) > 0]\n",
        "\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        nml_connects[locs[0]],\n",
        "        nml_connects[locs[1]],\n",
        "        assume_unique=True,\n",
        "        return_indices=True)\n",
        "    \n",
        "    bn_cont.append(cont_many)\n",
        "    nml_connects.append(np.concatenate((\n",
        "      np.delete(nml_connects[locs[0]], A_cont),\n",
        "      np.delete(nml_connects[locs[1]], B_cont))))\n",
        "    del nml_connects[locs[1]]\n",
        "    del nml_connects[locs[0]]\n",
        "    nml_order = np.delete(nml_order, np.intersect1d(nml_order, \n",
        "                                                    cont_many, \n",
        "                                                    return_indices=True)[1])\n",
        "\n",
        "  # compute partial trace costs\n",
        "  nml_connects = keep_connects\n",
        "  pt_costs = []\n",
        "  for pt_labs in pt_cont:\n",
        "    for count, sublist in enumerate(nml_connects):\n",
        "      pt_inds, pt_locs0, _ = np.intersect1d(\n",
        "          sublist, pt_labs, return_indices=True)\n",
        "      if len(pt_inds) > 0:\n",
        "        sublist = np.delete(sublist, pt_locs0)\n",
        "        cont_cost = np.delete(nml_dims[count], pt_locs0)\n",
        "        _, pt_locs1, _ = np.intersect1d(sublist, pt_labs, return_indices=True)\n",
        "        \n",
        "        nml_connects[count] = np.delete(sublist, pt_locs1)\n",
        "        nml_dims[count] = np.delete(cont_cost, pt_locs1)\n",
        "        break\n",
        "    pt_costs.append(cont_cost)\n",
        "    \n",
        "  # compute binary contraction costs\n",
        "  bn_costs = []\n",
        "  for bn_labs in bn_cont:\n",
        "    locs = [ele for ele in range(len(nml_connects)) if \n",
        "            sum(nml_connects[ele] == bn_labs[0]) > 0]\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        nml_connects[locs[0]],\n",
        "        nml_connects[locs[1]],\n",
        "        assume_unique=True,\n",
        "        return_indices=True)\n",
        "\n",
        "    nml_connects.append(np.concatenate((\n",
        "        np.delete(nml_connects[locs[0]], A_cont),\n",
        "        np.delete(nml_connects[locs[1]], B_cont))))\n",
        "    bn_costs.append(np.concatenate((\n",
        "        np.delete(nml_dims[locs[0]], A_cont), nml_dims[locs[1]])))\n",
        "    nml_dims.append(np.concatenate((\n",
        "        np.delete(nml_dims[locs[0]], A_cont),\n",
        "        np.delete(nml_dims[locs[1]], B_cont))))\n",
        "\n",
        "    del nml_connects[locs[1]]\n",
        "    del nml_connects[locs[0]]\n",
        "    del nml_dims[locs[1]]\n",
        "    del nml_dims[locs[0]]\n",
        "\n",
        "  # tally the total partial trace costs\n",
        "  is_symbolic = False\n",
        "  int_pt_costs = []\n",
        "  fin_pt_costs = []\n",
        "  for cost in pt_costs:\n",
        "    uni_dims = np.unique(cost)\n",
        "    str_cost = ''\n",
        "    int_cost = 1\n",
        "    for dim in uni_dims:\n",
        "      degen = sum(cost == dim)\n",
        "      if rev_dim_dict is None:\n",
        "        value = dim\n",
        "      else:\n",
        "        value = rev_dim_dict[dim]\n",
        "\n",
        "      if isinstance(value, str):\n",
        "        str_cost += '(' + str(value) + '^' + str(degen) + ')'\n",
        "        is_symbolic = True\n",
        "      else:\n",
        "        int_cost = int_cost * value**degen\n",
        "      \n",
        "    int_pt_costs.append(int_cost)\n",
        "    if int_cost > 1:\n",
        "      if int_cost > 1000:\n",
        "        if len(str_cost) > 0:\n",
        "          fin_pt_costs.append(\"{:.1e}\".format(int_cost) + '*' + str_cost)\n",
        "        else:\n",
        "          fin_pt_costs.append(int_cost)\n",
        "      else:\n",
        "        if len(str_cost) > 0:\n",
        "          fin_pt_costs.append(str(int_cost) + '*' + str_cost)\n",
        "        else:\n",
        "          fin_pt_costs.append(int_cost)\n",
        "    else:\n",
        "      fin_pt_costs.append(str_cost)\n",
        "\n",
        "  # tally the total binary contraction costs\n",
        "  int_bn_costs = []\n",
        "  fin_bn_costs = []\n",
        "  for cost in bn_costs:\n",
        "    uni_dims = np.unique(cost)\n",
        "    str_cost = ''\n",
        "    int_cost = 1\n",
        "    for dim in uni_dims:\n",
        "      degen = sum(cost == dim)\n",
        "      if rev_dim_dict is None:\n",
        "        value = dim\n",
        "      else:\n",
        "        value = rev_dim_dict[dim]\n",
        "\n",
        "      if isinstance(value, str):\n",
        "        str_cost += '(' + str(value) + '^' + str(degen) + ')'\n",
        "        is_symbolic = True\n",
        "      else:\n",
        "        int_cost = int_cost * value**degen\n",
        "\n",
        "    int_bn_costs.append(int_cost)\n",
        "    if int_cost > 1:\n",
        "      if int_cost > 1000:\n",
        "        if len(str_cost) > 0:\n",
        "          fin_bn_costs.append(\"{:.1e}\".format(int_cost, \"e\") + '*' + str_cost)\n",
        "        else:\n",
        "          fin_bn_costs.append(int_cost)\n",
        "      else:\n",
        "        if len(str_cost) > 0:\n",
        "          fin_bn_costs.append(str(int_cost) + '*' + str_cost)\n",
        "        else:\n",
        "          fin_bn_costs.append(int_cost)\n",
        "    else:\n",
        "      fin_bn_costs.append(str_cost)\n",
        "\n",
        "  if return_pt:\n",
        "    return fin_bn_costs, fin_pt_costs\n",
        "  else:\n",
        "    return fin_bn_costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7QqkwSX5Z77"
      },
      "source": [
        "def check_network(connects, dims, order, rev_dict=None, rev_dim_dict=None):\n",
        "  \"\"\" \n",
        "  Check the validity of a network. Require input `connects` to already be in\n",
        "  canonical form (+ve ints for internal inds and -ve ints for external).\n",
        "  \"\"\"\n",
        "\n",
        "  flat_connect = np.concatenate(connects)\n",
        "  pos_ind = flat_connect[flat_connect > 0]\n",
        "  neg_ind = flat_connect[flat_connect < 0]\n",
        "\n",
        "  # check that lengths of lists match\n",
        "  if len(dims) != len(connects):\n",
        "    raise ValueError((\n",
        "        'Network definition error: mismatch between {n0} tensors given but {n1}'\n",
        "        ' index sublists given'.format(n0 = str(len(dims)), \n",
        "                                       n1 = str(len(connects)))))\n",
        "\n",
        "  # check that tensors have the right number of indices\n",
        "  for ele in range(len(dims)):\n",
        "    if len(dims[ele]) != len(connects[ele]):\n",
        "      raise ValueError(\n",
        "          'Network definition error: number of indices does not match number'\n",
        "          ' of labels on tensor {n0}: {n1}-indices versus {n2}-labels'.format(\n",
        "              n0 = str(ele),\n",
        "              n1 = str(len(dims[ele])),\n",
        "              n2 = str(len(connects[ele]))))\n",
        "\n",
        "  # check that contraction order is valid\n",
        "  if not np.array_equal(np.sort(order), np.unique(pos_ind)):\n",
        "    raise ValueError('Network definition error: invalid contraction order')\n",
        "\n",
        "  # check that positive indices are valid and contracted tensor dimensions match\n",
        "  flat_dims = np.array([item for sublist in dims for item in sublist])\n",
        "  for ind in np.unique(pos_ind):\n",
        "    if sum(pos_ind == ind) == 1:\n",
        "      if rev_dict is not None:\n",
        "        ind_temp = rev_dict[ind]\n",
        "      else:\n",
        "        ind_temp = ind\n",
        "\n",
        "      raise ValueError(\n",
        "        'Network definition error: only one index labelled {n0}'\n",
        "        .format(n0 = \"`\" + str(ind_temp) + \"`\"))\n",
        "    elif sum(pos_ind == ind) > 2:\n",
        "      if rev_dict is not None:\n",
        "        ind_temp = rev_dict[ind]\n",
        "      else:\n",
        "        ind_temp = ind\n",
        "\n",
        "      raise ValueError(\n",
        "        'Network definition error: more than two indices labelled {n0}'\n",
        "        .format(n0 = \"`\" + str(ind_temp) + \"`\"))\n",
        "\n",
        "    cont_dims = flat_dims[flat_connect == ind]\n",
        "    if cont_dims[0] != cont_dims[1]:\n",
        "      if rev_dim_dict is not None:\n",
        "        d0 = rev_dim_dict[cont_dims[0]]\n",
        "        d1 = rev_dim_dict[cont_dims[1]]\n",
        "      else:\n",
        "        d0 = cont_dims[0]\n",
        "        d1 = cont_dims[1]\n",
        "\n",
        "      if rev_dict is not None:\n",
        "        ind_temp = rev_dict[ind]\n",
        "      else:\n",
        "        ind_temp = ind\n",
        "\n",
        "      raise ValueError(\n",
        "          'Network definition error: tensor dimension mismatch on'\n",
        "          ' index labelled {n0}: dim-{n1} versus dim-{n2}'\n",
        "          .format(n0 = \"`\" + str(ind_temp) + \"`\", \n",
        "                  n1 = str(d0), \n",
        "                  n2 = str(d1)))\n",
        "\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}