{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "network_solve.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1zMcNAAzuOXgcV970jdmj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gevenbly/TensorAlgs/blob/main/network_solve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ep0ftZwoorh"
      },
      "source": [
        "import numpy as np\n",
        "from typing import Optional, List, Union, Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Z_t-wAvCDN"
      },
      "source": [
        "def call_solver(tensors: Union[List[np.ndarray], List[tuple]],\n",
        "                labels: List[List[int]],\n",
        "                max_branch: Optional[int] = None,\n",
        "                order: Optional[List[int]] = None):\n",
        "  \"\"\"\n",
        "  Solve for the contraction order of a tensor network (encoded in the `ncon`\n",
        "  syntax) that minimizes the computational cost.\n",
        "  Args:\n",
        "    tensors: a list of the tensors in the network or a list of the shapes of\n",
        "      the tensors in the network.\n",
        "    labels: list of the tensor connections (in standard `ncon` format).\n",
        "    max_branch: maximum number of contraction paths to search at each step.\n",
        "  Returns:\n",
        "    np.ndarray: the cheapest contraction order found (in ncon format).\n",
        "    float: the cost of the network contraction, given as log10(total_FLOPS).\n",
        "    bool: specifies if contraction order is guaranteed optimal.\n",
        "  \"\"\"\n",
        "  # extract tensor shapes if necessary\n",
        "  if not isinstance(tensors[0], tuple):\n",
        "    dims = [tensor.shape for tensor in tensors]\n",
        "  else:\n",
        "    dims = tensors\n",
        "\n",
        "  # build log-adjacency matrix\n",
        "  log_adj = ncon_to_weighted_adj(dims, labels)\n",
        "\n",
        "  # run search algorithm\n",
        "  node_order, costs, is_optimal = full_solve_complete(\n",
        "      log_adj, max_branch=max_branch)\n",
        "\n",
        "  # put contraction order back into ncon format\n",
        "  order = ord_to_ncon(labels, node_order)\n",
        "\n",
        "  return order, costs, is_optimal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Am8IYJC7AfE"
      },
      "source": [
        "def ord_to_ncon(labels: List[List[int]], orders: np.ndarray):\n",
        "  \"\"\"\n",
        "  Produces a `ncon` compatible index contraction order from the sequence of\n",
        "  pairwise contractions.\n",
        "  Args:\n",
        "    labels: list of the tensor connections (in standard `ncon` format).\n",
        "    orders: array of dim (2,N-1) specifying the set of N-1 pairwise\n",
        "      tensor contractions.\n",
        "  Returns:\n",
        "    np.ndarray: the contraction order (in `ncon` format).\n",
        "  \"\"\"\n",
        "\n",
        "  N = len(labels)\n",
        "  orders = orders.reshape(2, N - 1)\n",
        "  new_labels = [np.array(labels[i]) for i in range(N)]\n",
        "  con_order = np.zeros([0], dtype=int)\n",
        "\n",
        "  # remove all partial trace indices\n",
        "  for counter, temp_label in enumerate(new_labels):\n",
        "    uni_inds, counts = np.unique(temp_label, return_counts=True)\n",
        "    tr_inds = uni_inds[np.flatnonzero(counts == 2)]\n",
        "    con_order = np.concatenate((con_order, tr_inds))\n",
        "    new_labels[counter] = temp_label[np.isin(temp_label, uni_inds[counts == 1])]\n",
        "\n",
        "  for i in range(N - 1):\n",
        "    # find common indices between tensor pair\n",
        "    cont_many, A_cont, B_cont = np.intersect1d(\n",
        "        new_labels[orders[0, i]], new_labels[orders[1, i]], return_indices=True)\n",
        "    temp_labels = np.append(\n",
        "        np.delete(new_labels[orders[0, i]], A_cont),\n",
        "        np.delete(new_labels[orders[1, i]], B_cont))\n",
        "    con_order = list(np.concatenate((con_order, cont_many), axis=0))\n",
        "\n",
        "    # build new set of labels\n",
        "    new_labels[orders[0, i]] = temp_labels\n",
        "    del new_labels[orders[1, i]]\n",
        "\n",
        "  return con_order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXbCbjS267uK"
      },
      "source": [
        "def ncon_to_weighted_adj(dims: List[Tuple], labels: List[List[int]]):\n",
        "  \"\"\"\n",
        "  Create a log-adjacency matrix, where element [i,j] is the log10 of the total\n",
        "  dimension of the indices connecting ith and jth tensors, for a network\n",
        "  defined in the `ncon` syntax.\n",
        "  Args:\n",
        "    dims: list containing the shape of each tensor in the network.\n",
        "    labels: list of the tensor connections (in standard `ncon` format).\n",
        "  Returns:\n",
        "    np.ndarray: the log-adjacency matrix.\n",
        "  \"\"\"\n",
        "  # process inputs\n",
        "  N = len(labels)\n",
        "  ranks = [len(labels[i]) for i in range(N)]\n",
        "  flat_labels = np.hstack([labels[i] for i in range(N)])\n",
        "  tensor_counter = np.hstack(\n",
        "      [i * np.ones(ranks[i], dtype=int) for i in range(N)])\n",
        "  index_counter = np.hstack([np.arange(ranks[i]) for i in range(N)])\n",
        "\n",
        "  # build log-adjacency index-by-index\n",
        "  log_adj = np.zeros([N, N])\n",
        "  unique_labels = np.unique(flat_labels)\n",
        "  for ele in unique_labels:\n",
        "    # identify tensor/index location of each edge\n",
        "    tnr = tensor_counter[flat_labels == ele]\n",
        "    ind = index_counter[flat_labels == ele]\n",
        "    if len(ind) == 1:  # external index\n",
        "      log_adj[tnr[0], tnr[0]] += np.log10(dims[tnr[0]][ind[0]])\n",
        "    elif len(ind) == 2:  # internal index\n",
        "      if tnr[0] != tnr[1]:  # ignore partial traces\n",
        "        log_adj[tnr[0], tnr[1]] += np.log10(dims[tnr[0]][ind[0]])\n",
        "        log_adj[tnr[1], tnr[0]] += np.log10(dims[tnr[0]][ind[0]])\n",
        "\n",
        "  return log_adj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjiKu6wYpFh-"
      },
      "source": [
        "def full_solve_complete(log_adj: np.ndarray,\n",
        "                        cost_bound: Optional[int] = None,\n",
        "                        max_branch: Optional[int] = None):\n",
        "  \"\"\"\n",
        "  Solve for optimal contraction path of a network encoded as a log-adjacency\n",
        "  matrix via a full search.\n",
        "  Args:\n",
        "    log_adj: matrix where element [i,j] is the log10 of the total dimension\n",
        "      of the indices connecting ith and jth tensors.\n",
        "    cost_bound: upper cost threshold for discarding paths, in log10(FLOPS).\n",
        "    max_branch: bound for the total number of paths to retain.\n",
        "  Returns:\n",
        "    np.ndarray: the cheapest contraction order found.\n",
        "    float: the cost of the network contraction, given as log10(total_FLOPS).\n",
        "    bool: specifies if contraction order is guaranteed optimal.\n",
        "  \"\"\"\n",
        "  tol = 1e-6  # tolerance for float comparison\n",
        "  # start by trying both greedy algorithms\n",
        "  order0, cost0 = greedy_size_solve(log_adj)\n",
        "  order1, cost1 = greedy_cost_solve(log_adj)\n",
        "  if cost0 < cost1:\n",
        "    order_greedy = order0\n",
        "    cost_greedy = cost0\n",
        "  else:\n",
        "    order_greedy = order1\n",
        "    cost_greedy = cost1\n",
        "\n",
        "  if max_branch == 1:\n",
        "    # return results from greedy\n",
        "    order_was_found = False\n",
        "  else:\n",
        "    # initialize arrays\n",
        "    N = log_adj.shape[0]\n",
        "    costs = np.zeros([1, 0])\n",
        "    groups = np.array(2**np.arange(N), dtype=np.uint64).reshape(N, 1)\n",
        "    orders = np.zeros([2, 0, 1], dtype=int)\n",
        "\n",
        "    # try full algorithm (using cost_bound from greedy)\n",
        "    cost_bound = cost_greedy + tol\n",
        "    total_truncated = 0\n",
        "    order_was_found = True\n",
        "    for _ in range(N - 1):\n",
        "      log_adj, costs, groups, orders, num_truncated = _full_solve_single(\n",
        "          log_adj,\n",
        "          costs,\n",
        "          groups,\n",
        "          orders,\n",
        "          cost_bound=cost_bound,\n",
        "          max_branch=max_branch)\n",
        "      if log_adj.size == 0:\n",
        "        # no paths found within the cost-bound\n",
        "        order_was_found = False\n",
        "        break\n",
        "      total_truncated = total_truncated + num_truncated\n",
        "\n",
        "  if order_was_found:\n",
        "    # return result from full algorithm\n",
        "    is_optimal = (total_truncated == 0)\n",
        "    return orders.reshape(2, N - 1), costs.item(), is_optimal\n",
        "\n",
        "  # return result from greedy algorithm\n",
        "  is_optimal = False\n",
        "  return order_greedy, cost_greedy, is_optimal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC6aTQO4ovV1"
      },
      "source": [
        "def greedy_cost_solve(log_adj_in: np.ndarray):\n",
        "  \"\"\"\n",
        "  Solve for the contraction order of a tensor network (encoded as a\n",
        "  log-adjacency matrix) using a greedy algorithm that minimizes the\n",
        "  contraction cost at each step.\n",
        "  Args:\n",
        "    log_adj_in: matrix where element [i,j] is the log10 of the total dimension\n",
        "      of the indices connecting ith and jth tensors.\n",
        "  Returns:\n",
        "    np.ndarray: cheapest contraction order found, specified as a sequence of\n",
        "      binary contractions.\n",
        "    float: the cost of the network contraction, given as log10(total_FLOPS).\n",
        "  \"\"\"\n",
        "  tol = 1e-6  # tolerance for float comparison\n",
        "  N = log_adj_in.shape[0]\n",
        "  log_adj = log_adj_in.copy().reshape(N, N)\n",
        "  orders = np.zeros([2, 0], dtype=int)\n",
        "  costs = None\n",
        "\n",
        "  for _ in range(N - 1):\n",
        "    # compute tensor dims and costs\n",
        "    N = log_adj.shape[0]\n",
        "    dims = np.sum(log_adj, axis=0).reshape(N)\n",
        "    comb_dims = np.add.outer(dims, dims)\n",
        "    single_cost = comb_dims - log_adj\n",
        "\n",
        "    # penalize trivial contractions and self-contractions\n",
        "    triv_conts = (log_adj < tol)\n",
        "    trimmed_costs = single_cost + np.max(single_cost.flatten()) * triv_conts\n",
        "    trimmed_costs = trimmed_costs + np.max(trimmed_costs.flatten()) * np.eye(N)\n",
        "\n",
        "    # find best contraction\n",
        "    tensors_to_contract = np.divmod(np.argmin(trimmed_costs), N)\n",
        "    i = max(tensors_to_contract)\n",
        "    j = min(tensors_to_contract)\n",
        "\n",
        "    # build new log adjacency\n",
        "    log_adj[j, j] = log_adj[j, j] - 2 * log_adj[j, i]\n",
        "    log_adj[j, :] = log_adj[j, :] + log_adj[i, :]\n",
        "    log_adj[:, j] = log_adj[:, j] + log_adj[:, i]\n",
        "    log_adj = np.delete(log_adj, i, axis=0)\n",
        "    log_adj = np.delete(log_adj, i, axis=1)\n",
        "\n",
        "    # build new orders\n",
        "    orders = np.hstack((orders, np.asarray(tensors_to_contract).reshape(2, 1)))\n",
        "\n",
        "    # tally the cost\n",
        "    if costs is None:\n",
        "      costs = single_cost[i, j]\n",
        "    else:\n",
        "      costs = costs + np.log10(1 + 10**(single_cost[i, j] - costs))\n",
        "\n",
        "  return orders, costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7hSnqczo9UV"
      },
      "source": [
        "def greedy_size_solve(log_adj_in: np.ndarray):\n",
        "  \"\"\"\n",
        "  Solve for the contraction order of a tensor network (encoded as a\n",
        "  log-adjacency matrix) using a greedy algorithm that minimizes the\n",
        "  intermediate tensor sizes.\n",
        "  Args:\n",
        "    log_adj_in: matrix where element [i,j] is the log10 of the total dimension\n",
        "      of the indices connecting ith and jth tensors.\n",
        "  Returns:\n",
        "    np.ndarray: cheapest contraction order found, specified as a sequence of\n",
        "      binary contractions.\n",
        "    float: the cost of the network contraction, given as log10(total_FLOPS).\n",
        "  \"\"\"\n",
        "  tol = 1e-6  # tolerance for float comparison\n",
        "  N0 = log_adj_in.shape[0]\n",
        "  log_adj = log_adj_in.copy().reshape(N0, N0)\n",
        "  orders = np.zeros([2, 0], dtype=int)\n",
        "  costs = None\n",
        "\n",
        "  for _ in range(N0 - 1):\n",
        "    # compute tensor dims\n",
        "    N = log_adj.shape[0]\n",
        "    dims = np.sum(log_adj, axis=0).reshape(N)\n",
        "    comb_dims = np.add.outer(dims, dims)\n",
        "\n",
        "    # compute contraction costs and new dims\n",
        "    single_cost = comb_dims - log_adj\n",
        "    new_dims = comb_dims - 2 * log_adj\n",
        "    new_dims = new_dims + np.max(new_dims.flatten()) * np.eye(N)\n",
        "\n",
        "    # compute maximum dim of tensor in contraction\n",
        "    temp_mat = np.kron(dims, np.ones([N, 1]))\n",
        "    max_dim = np.maximum(temp_mat, temp_mat.T)\n",
        "    dim_change = ((1 / tol) * (new_dims - max_dim)).astype(int)\n",
        "\n",
        "    # compute coords of minimal dim increase\n",
        "    xcoord, ycoord = np.where(dim_change == np.min(dim_change.flatten()))\n",
        "    upper_tri = (xcoord < ycoord)\n",
        "    xcoord = xcoord[upper_tri]\n",
        "    ycoord = ycoord[upper_tri]\n",
        "\n",
        "    # find contraction with minimal cost\n",
        "    all_costs = np.array(\n",
        "        [single_cost[xcoord[i], ycoord[i]] for i in range(len(xcoord))])\n",
        "    cont_dims = np.array(\n",
        "        [log_adj[xcoord[i], ycoord[i]] for i in range(len(xcoord))])\n",
        "    if max(cont_dims) > 0:  # prioritise non-trivial contractions\n",
        "      all_costs[cont_dims == 0] += max(all_costs) + 1\n",
        "\n",
        "    cheapest_pos = np.argmin(all_costs)\n",
        "    i = ycoord[cheapest_pos]\n",
        "    j = xcoord[cheapest_pos]\n",
        "\n",
        "    # build new log adjacency\n",
        "    log_adj[j, j] = log_adj[j, j] - 2 * log_adj[j, i]\n",
        "    log_adj[j, :] = log_adj[j, :] + log_adj[i, :]\n",
        "    log_adj[:, j] = log_adj[:, j] + log_adj[:, i]\n",
        "    log_adj = np.delete(log_adj, i, axis=0)\n",
        "    log_adj = np.delete(log_adj, i, axis=1)\n",
        "\n",
        "    # build new orders\n",
        "    orders = np.hstack((orders, np.asarray([j, i]).reshape(2, 1)))\n",
        "\n",
        "    # tally the cost\n",
        "    if costs is None:\n",
        "      costs = single_cost[i, j]\n",
        "    else:\n",
        "      costs = costs + np.log10(1 + 10**(single_cost[i, j] - costs))\n",
        "\n",
        "  return orders, costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3-rx0YcpPoj"
      },
      "source": [
        "def _full_solve_single(log_adj: np.ndarray,\n",
        "                       costs: np.ndarray,\n",
        "                       groups: np.ndarray,\n",
        "                       orders: np.ndarray,\n",
        "                       cost_bound: Optional[int] = None,\n",
        "                       max_branch: Optional[int] = None,\n",
        "                       allow_outer: Optional[bool] = False):\n",
        "  \"\"\"\n",
        "  Solve for the most-likely contraction step given a set of networks encoded\n",
        "  as log-adjacency matrices. Uses an algorithm that searches multiple (or,\n",
        "  potentially, all viable paths) as to minimize the total contraction cost.\n",
        "  Args:\n",
        "    log_adj: an np.ndarray of log-adjacency matrices of dim (N,N,m), with `N`\n",
        "      the number of tensors and `m` the number of (intermediate) networks.\n",
        "    costs: np.ndarray of length `m` detailing to prior cost of each network.\n",
        "    groups: np.ndarray of dim (N,m) providing an id-tag for each network,\n",
        "      based on a power-2 encoding.\n",
        "    orders: np.ndarray of dim (2,t,m) detailing the pairwise contraction\n",
        "      history of each network from the previous `t` contraction steps.\n",
        "    cost_bound: upper cost threshold for discarding paths, in log10(FLOPS).\n",
        "    max_branch: bound for the total number of paths to retain.\n",
        "    allow_outer: sets whether outer products are allowed.\n",
        "  Returns:\n",
        "    np.ndarray: new set of `log_adj` matrices.\n",
        "    np.ndarray: new set of `costs`.\n",
        "    np.ndarray: new set of `groups`.\n",
        "    np.ndarray: new set of `orders`.\n",
        "    int: total number of potentially viable paths that were trimmed.\n",
        "  \"\"\"\n",
        "  tol = 1e-6  # tolerance for float comparison\n",
        "\n",
        "  # set threshold required to trigger compression routine\n",
        "  if max_branch is None:\n",
        "    mid_kept = 10000\n",
        "  else:\n",
        "    mid_kept = max_branch\n",
        "\n",
        "  # initialize outputs\n",
        "  N = log_adj.shape[0]\n",
        "  if log_adj.ndim == 2:\n",
        "    log_adj = log_adj.reshape(N, N, 1)\n",
        "  final_adj = np.zeros([N - 1, N - 1, 0])\n",
        "  final_costs = np.zeros([1, 0])\n",
        "  final_groups = np.zeros([N - 1, 0], dtype=np.uint64)\n",
        "  final_orders = np.zeros([2, orders.shape[1] + 1, 0], dtype=int)\n",
        "  final_stable = np.zeros([1, 0], dtype=bool)\n",
        "  total_truncated = 0\n",
        "\n",
        "  only_outer_exist = not allow_outer\n",
        "  none_inbounds = True\n",
        "\n",
        "  # try to contract j-th tensor with i-th tensor (j<i)\n",
        "  for i in range(1, N):\n",
        "    for j in range(i):\n",
        "\n",
        "      if not allow_outer:\n",
        "        # only attempt non-trivial contractions\n",
        "        new_pos = np.flatnonzero(log_adj[j, i, :] > 0)\n",
        "        num_kept = len(new_pos)\n",
        "      else:\n",
        "        new_pos = np.arange(log_adj.shape[2])\n",
        "        num_kept = len(new_pos)\n",
        "\n",
        "      if num_kept > 0:\n",
        "        only_outer_exist = False\n",
        "\n",
        "        # dims of tensors and cost of contraction\n",
        "        dims = np.sum(log_adj[:, :, new_pos], axis=0).reshape(N, num_kept)\n",
        "        comb_dims = dims[j, :] + dims[i, :]\n",
        "        single_cost = np.reshape(comb_dims - log_adj[j, i, new_pos],\n",
        "                                 [1, num_kept])\n",
        "        if costs.size == 0:\n",
        "          new_costs = single_cost\n",
        "        else:\n",
        "          prev_cost = costs[0, new_pos]\n",
        "          new_costs = prev_cost + np.log10(1 + 10**(single_cost - prev_cost))\n",
        "\n",
        "        if cost_bound is not None:\n",
        "          # only keep contractions under the cost bound\n",
        "          pos_under_bound = new_costs.flatten() < cost_bound\n",
        "          new_pos = new_pos[pos_under_bound]\n",
        "          num_kept = len(new_pos)\n",
        "\n",
        "          new_costs = new_costs[0, pos_under_bound].reshape(1, num_kept)\n",
        "\n",
        "      if num_kept > 0:\n",
        "        none_inbounds = False\n",
        "\n",
        "        # order the costs\n",
        "        cost_order = np.argsort(new_costs).flatten()\n",
        "        sorted_pos = new_pos[cost_order]\n",
        "\n",
        "        # identify identical networks\n",
        "        new_groups = groups[:, sorted_pos]\n",
        "        new_groups[j, :] = new_groups[j, :] + new_groups[i, :]\n",
        "        new_groups = np.delete(new_groups, i, axis=0)\n",
        "        new_groups, temp_pos = np.unique(new_groups, return_index=True, axis=1)\n",
        "\n",
        "        new_costs = new_costs[:, cost_order[temp_pos]]\n",
        "        new_pos = sorted_pos[temp_pos]\n",
        "        num_kept = len(new_pos)\n",
        "\n",
        "        # new log adjacency\n",
        "        new_adj = log_adj[:, :, new_pos]\n",
        "        new_adj[j, j, :] = new_adj[j, j, :] - 2 * new_adj[j, i, :]\n",
        "        new_adj[j, :, :] = new_adj[j, :, :] + new_adj[i, :, :]\n",
        "        new_adj[:, j, :] = new_adj[:, j, :] + new_adj[:, i, :]\n",
        "        new_adj = np.delete(new_adj, i, axis=0)\n",
        "        new_adj = np.delete(new_adj, i, axis=1)\n",
        "\n",
        "        # new orders\n",
        "        prev_orders = orders[:, :, new_pos]\n",
        "        next_orders = np.vstack([\n",
        "            j * np.ones(len(new_pos), dtype=int),\n",
        "            i * np.ones(len(new_pos), dtype=int)\n",
        "        ]).reshape(2, 1, len(new_pos))\n",
        "        new_orders = np.concatenate((prev_orders, next_orders), axis=1)\n",
        "\n",
        "        # new_stable\n",
        "        dims = np.sum(log_adj[:, :, new_pos], axis=0).reshape(N, num_kept)\n",
        "        comb_dims = dims[j, :] + dims[i, :]\n",
        "        final_dims = np.reshape(comb_dims - 2 * log_adj[j, i, new_pos],\n",
        "                                [1, num_kept])\n",
        "\n",
        "        # include a fudge factor to avoid rounding errors\n",
        "        stable_pos = final_dims < (np.maximum(dims[j, :], dims[i, :]) + tol)\n",
        "\n",
        "        final_adj = np.concatenate((final_adj, new_adj), axis=2)\n",
        "        final_costs = np.concatenate((final_costs, new_costs), axis=1)\n",
        "        final_groups = np.concatenate((final_groups, new_groups), axis=1)\n",
        "        final_orders = np.concatenate((final_orders, new_orders), axis=2)\n",
        "        final_stable = np.concatenate((final_stable, stable_pos), axis=1)\n",
        "\n",
        "        # if number of intermediates too large then trigger compression routine\n",
        "        if final_costs.size > mid_kept:\n",
        "          temp_pos, num_truncated = _reduce_nets(\n",
        "              final_costs, final_groups, final_stable, max_branch=max_branch)\n",
        "          final_adj = final_adj[:, :, temp_pos]\n",
        "          final_costs = final_costs[:, temp_pos]\n",
        "          final_groups = final_groups[:, temp_pos]\n",
        "          final_orders = final_orders[:, :, temp_pos]\n",
        "          final_stable = final_stable[:, temp_pos]\n",
        "          total_truncated = total_truncated + num_truncated\n",
        "\n",
        "  if not only_outer_exist:\n",
        "    if none_inbounds:\n",
        "      # no orders found under the cost bound; return trivial\n",
        "      return np.zeros(0), np.zeros(0), np.zeros(0), np.zeros(0), 0\n",
        "\n",
        "  if only_outer_exist:  # network contains only outer products\n",
        "    # re-solve with outer products enabled\n",
        "    return _full_solve_single(\n",
        "        log_adj,\n",
        "        costs,\n",
        "        groups,\n",
        "        orders,\n",
        "        cost_bound=cost_bound,\n",
        "        max_branch=max_branch,\n",
        "        allow_outer=True)\n",
        "\n",
        "  # compress outputs\n",
        "  temp_pos = _reduce_nets(final_costs, final_groups, final_stable)[0]\n",
        "  final_adj = final_adj[:, :, temp_pos]\n",
        "  final_costs = final_costs[:, temp_pos]\n",
        "  final_groups = final_groups[:, temp_pos]\n",
        "  final_orders = final_orders[:, :, temp_pos]\n",
        "  final_stable = final_stable[:, temp_pos]\n",
        "  return final_adj, final_costs, final_groups, final_orders, total_truncated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfTf9MqZoj8G"
      },
      "source": [
        "def _reduce_nets(costs: np.ndarray,\n",
        "                 groups: np.ndarray,\n",
        "                 stable: np.ndarray,\n",
        "                 max_branch: Optional[int] = None):\n",
        "  \"\"\"\n",
        "  Reduce from `m` starting paths smaller number of paths by first (i)\n",
        "  identifying any equivalent networks then (ii) trimming the most expensive\n",
        "  paths.\n",
        "  Args:\n",
        "    costs: np.ndarray of length `m` detailing to prior cost of each network.\n",
        "    groups: np.ndarray of dim (N,m) providing an id-tag for each network,\n",
        "      based on a power-2 encoding.\n",
        "    stable: np.ndarray of dim (m) denoting which paths were size-stable.\n",
        "    max_branch: bound for the total number of paths to retain.\n",
        "  Returns:\n",
        "    np.ndarray: index positions of the kept paths.\n",
        "    int: total number of potentially viable paths that were trimmed.\n",
        "  \"\"\"\n",
        "\n",
        "  # sort according to the costs\n",
        "  new_pos = np.argsort(costs).flatten()\n",
        "\n",
        "  # identify and remove identical networks\n",
        "  temp_pos = np.unique(groups[:, new_pos], return_index=True, axis=1)[1]\n",
        "  orig_kept = len(temp_pos)\n",
        "  new_pos = new_pos[temp_pos]\n",
        "  num_truncated = 0\n",
        "\n",
        "  if max_branch is not None:\n",
        "    if orig_kept > max_branch:\n",
        "      # re-sort according to the costs\n",
        "      cost_order = np.argsort(costs[:, new_pos]).flatten()\n",
        "      new_pos = new_pos[cost_order]\n",
        "\n",
        "      # reserve some percertage for size-stable contractions\n",
        "      preserve_ratio = 0.2\n",
        "      num_stable = int(np.ceil(max_branch * preserve_ratio))\n",
        "      num_cheapest = int(np.ceil(max_branch * (1 - preserve_ratio)))\n",
        "\n",
        "      stable_pos = np.flatnonzero(stable[0, new_pos[num_cheapest:]])\n",
        "      temp_pos = np.concatenate(\n",
        "          (np.arange(num_cheapest),\n",
        "           stable_pos[:min(len(stable_pos), num_stable)] + num_cheapest),\n",
        "          axis=0)\n",
        "      new_pos = new_pos[temp_pos]\n",
        "      num_truncated = orig_kept - len(new_pos)\n",
        "\n",
        "  return new_pos, num_truncated"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}